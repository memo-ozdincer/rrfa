{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ebe8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/homebrew/anaconda3/lib/python3.13/site-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (1.2.1)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: typer-slim in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/homebrew/anaconda3/lib/python3.13/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.1.8)\n",
      "‚úÖ Project structure created at ..\n",
      "‚úÖ Database path: ../data/db/unified.db\n",
      "‚úÖ All directories ready\n"
     ]
    }
   ],
   "source": [
    "!pip3 install datasets\n",
    "# ============================================================================\n",
    "# CELL 1: SETUP & ENVIRONMENT\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "import shutil\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Dataset loading\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Create project structure\n",
    "BASE_DIR = Path(\"../\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "DB_DIR = DATA_DIR / \"db\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# Create directories\n",
    "for directory in [DATA_DIR, DB_DIR, PROCESSED_DIR, \n",
    "                  DATA_DIR / \"weblinx\",\n",
    "                  DATA_DIR / \"webarena\" / \"config_files\",\n",
    "                  DATA_DIR / \"webarena\" / \"human_trajectories\",\n",
    "                  DATA_DIR / \"webarena\" / \"llm_trajectories_v2\",\n",
    "                  DATA_DIR / \"tau2\" / \"domains\",\n",
    "                  DATA_DIR / \"tau2\" / \"results\",\n",
    "                  DATA_DIR / \"tau2\" / \"user_simulator\"]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize SQLite database\n",
    "DB_PATH = DB_DIR / \"unified.db\"\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(f\"‚úÖ Project structure created at {BASE_DIR}\")\n",
    "print(f\"‚úÖ Database path: {DB_PATH}\")\n",
    "print(f\"‚úÖ All directories ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3f1b1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['demo', 'turn', 'action', 'action_history', 'utterances', 'candidates', 'clean_html', 'viewport']\n",
      "First sample keys: dict_keys(['demo', 'turn', 'action', 'action_history', 'utterances', 'candidates', 'clean_html', 'viewport'])\n"
     ]
    }
   ],
   "source": [
    "# Add this right after loading weblinx_val\n",
    "print(\"Available columns:\", weblinx_val.column_names)\n",
    "print(\"First sample keys:\", weblinx_val[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fa57942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading WebLINX dataset from HuggingFace...\n",
      "‚úÖ Loaded 2126 validation samples\n",
      "üîç Columns found: ['demo', 'turn', 'action', 'action_history', 'utterances', 'candidates', 'clean_html', 'viewport']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f69e9d346494cf8a893760fb21c1129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e42d7e58f544985b7f24eed3ee409f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Inserted 100 WebLINX records\n",
      "  ‚úì Inserted 200 WebLINX records\n",
      "  ‚úì Inserted 300 WebLINX records\n",
      "  ‚úì Inserted 400 WebLINX records\n",
      "  ‚úì Inserted 500 WebLINX records\n",
      "  ‚úì Inserted 600 WebLINX records\n",
      "  ‚úì Inserted 700 WebLINX records\n",
      "  ‚úì Inserted 800 WebLINX records\n",
      "  ‚úì Inserted 900 WebLINX records\n",
      "  ‚úì Inserted 1000 WebLINX records\n",
      "‚úÖ WebLINX data inserted into database (1000 samples)\n",
      "‚úÖ Sample saved to ../data/processed/weblinx_sample.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: INGEST WEBLINX DATASET (CORRECTED KEYS)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üîÑ Loading WebLINX dataset from HuggingFace...\")\n",
    "\n",
    "# Load validation and test splits\n",
    "weblinx_val = load_dataset(\"McGill-NLP/weblinx\", split=\"validation\")\n",
    "weblinx_test_iid = load_dataset(\"McGill-NLP/weblinx\", split=\"test_iid\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(weblinx_val)} validation samples\")\n",
    "print(f\"üîç Columns found: {weblinx_val.column_names}\")\n",
    "\n",
    "# Download templates (for preprocessing)\n",
    "template_dir = DATA_DIR / \"weblinx\" / \"templates\"\n",
    "snapshot_download(\n",
    "    \"McGill-NLP/WebLINX\",\n",
    "    repo_type=\"dataset\",\n",
    "    allow_patterns=\"templates/*\",\n",
    "    local_dir=DATA_DIR / \"weblinx\"\n",
    ")\n",
    "\n",
    "# Create SQLite table for WebLINX\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS weblinx (\n",
    "        id TEXT PRIMARY KEY,\n",
    "        demo_id TEXT,\n",
    "        turn_id INT,\n",
    "        action TEXT,\n",
    "        action_history TEXT,\n",
    "        utterances TEXT,\n",
    "        candidates TEXT,\n",
    "        clean_html TEXT,\n",
    "        viewport TEXT,\n",
    "        source TEXT DEFAULT 'weblinx'\n",
    "    )\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# Insert WebLINX data (sample first 1000)\n",
    "weblinx_combined = weblinx_val.select(range(min(1000, len(weblinx_val))))\n",
    "\n",
    "for idx, sample in enumerate(weblinx_combined):\n",
    "    # CORRECT KEYS: 'demo' and 'turn' (not demo_id/turn_id)\n",
    "    demo_id = sample['demo']\n",
    "    turn_id = sample['turn']\n",
    "    \n",
    "    doc_id = f\"weblinx_{demo_id}_{turn_id}\"\n",
    "    \n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT OR REPLACE INTO weblinx VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (\n",
    "        doc_id,\n",
    "        demo_id,\n",
    "        turn_id,\n",
    "        str(sample.get('action', '')),\n",
    "        str(sample.get('action_history', '')),\n",
    "        str(sample.get('utterances', '')),\n",
    "        str(sample.get('candidates', '')),\n",
    "        str(sample.get('clean_html', ''))[:5000], # Truncate HTML\n",
    "        str(sample.get('viewport', '')),\n",
    "        'weblinx'\n",
    "    ))\n",
    "    \n",
    "    if (idx + 1) % 100 == 0:\n",
    "        print(f\"  ‚úì Inserted {idx + 1} WebLINX records\")\n",
    "\n",
    "conn.commit()\n",
    "print(f\"‚úÖ WebLINX data inserted into database ({len(weblinx_combined)} samples)\")\n",
    "\n",
    "# Save WebLINX JSON for reference\n",
    "# Save WebLINX JSON for reference\n",
    "weblinx_json_path = PROCESSED_DIR / \"weblinx_sample.json\"\n",
    "\n",
    "# FIX: dataset[:100] returns a dict of lists. \n",
    "# We need to iterate the dataset object to get rows.\n",
    "json_safe_sample = []\n",
    "\n",
    "# Iterate over the first 100 rows explicitly\n",
    "for i in range(min(100, len(weblinx_combined))):\n",
    "    row = weblinx_combined[i] # Accessing by index yields a dict (row)\n",
    "    # Convert all values to string to be safe for JSON\n",
    "    safe_row = {k: str(v) for k, v in row.items()}\n",
    "    json_safe_sample.append(safe_row)\n",
    "\n",
    "with open(weblinx_json_path, \"w\") as f:\n",
    "    json.dump(json_safe_sample, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Sample saved to {weblinx_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39c3b4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found WebArena config at ../data/webarena/config_files/test.raw.json\n",
      "‚úÖ Loaded 812 WebArena tasks\n",
      "‚úÖ WebArena tasks inserted into database\n",
      "‚úÖ Sample saved to ../data/processed/webarena_tasks_sample.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: INGEST WEBARENA CONFIG (test.raw.json)\n",
    "# ============================================================================\n",
    "# Assumes you've already downloaded test.raw.json manually\n",
    "\n",
    "webarena_config_path = DATA_DIR / \"webarena\" / \"config_files\" / \"test.raw.json\"\n",
    "\n",
    "if webarena_config_path.exists():\n",
    "    print(f\"‚úÖ Found WebArena config at {webarena_config_path}\")\n",
    "    \n",
    "    with open(webarena_config_path, 'r') as f:\n",
    "        webarena_tasks = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(webarena_tasks)} WebArena tasks\")\n",
    "    \n",
    "    # Create table for WebArena tasks\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS webarena_tasks (\n",
    "            task_id INTEGER PRIMARY KEY,\n",
    "            sites TEXT,\n",
    "            require_login BOOLEAN,\n",
    "            start_url TEXT,\n",
    "            intent TEXT,\n",
    "            intent_template TEXT,\n",
    "            eval_types TEXT,\n",
    "            reference_answers TEXT,\n",
    "            source TEXT DEFAULT 'webarena'\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    \n",
    "    # Insert WebArena tasks\n",
    "    for task in webarena_tasks:\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT OR REPLACE INTO webarena_tasks VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            task.get('task_id'),\n",
    "            str(task.get('sites', [])),\n",
    "            task.get('require_login', False),\n",
    "            task.get('start_url', ''),\n",
    "            task.get('intent', ''),\n",
    "            task.get('intent_template', ''),\n",
    "            str(task.get('eval', {}).get('eval_types', [])),\n",
    "            json.dumps(task.get('eval', {}).get('reference_answers', {})),\n",
    "            'webarena'\n",
    "        ))\n",
    "    \n",
    "    conn.commit()\n",
    "    print(f\"‚úÖ WebArena tasks inserted into database\")\n",
    "    \n",
    "    # Save sample\n",
    "    webarena_sample_path = PROCESSED_DIR / \"webarena_tasks_sample.json\"\n",
    "    with open(webarena_sample_path, \"w\") as f:\n",
    "        json.dump(webarena_tasks[:100], f, indent=2)\n",
    "    print(f\"‚úÖ Sample saved to {webarena_sample_path}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  WebArena config not found at {webarena_config_path}\")\n",
    "    print(f\"   Expected location: {webarena_config_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c5aea12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing WebArena human trajectories from ../data/webarena/human_trajectories...\n",
      "   Found 2 .trace directories\n",
      "‚úÖ Inserted 2 human trace records\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: PARSE WEBARENA HUMAN TRAJECTORIES (.trace format)\n",
    "# ============================================================================\n",
    "\n",
    "def parse_trace_directory(trace_dir: Path) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse .trace directory from human trajectories.\n",
    "    Structure:\n",
    "    - resources/\n",
    "    - trace.network\n",
    "    - trace.stacks\n",
    "    - trace.trace\n",
    "    \"\"\"\n",
    "    files_present = {\n",
    "        \"has_resources\": (trace_dir / \"resources\").exists(),\n",
    "        \"has_network\": (trace_dir / \"trace.network\").exists(),\n",
    "        \"has_stacks\": (trace_dir / \"trace.stacks\").exists(),\n",
    "        \"has_trace\": (trace_dir / \"trace.trace\").exists(),\n",
    "    }\n",
    "    \n",
    "    result = {\n",
    "        \"trace_dir\": trace_dir.name,\n",
    "        \"files\": files_present,\n",
    "        \"resource_count\": 0\n",
    "    }\n",
    "    \n",
    "    # Count resources\n",
    "    if files_present[\"has_resources\"]:\n",
    "        resource_files = list((trace_dir / \"resources\").glob(\"*\"))\n",
    "        result[\"resource_count\"] = len(resource_files)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create table for WebArena human trajectories\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS webarena_human_traces (\n",
    "        trace_id TEXT PRIMARY KEY,\n",
    "        trace_dir_name TEXT,\n",
    "        has_network BOOLEAN,\n",
    "        has_stacks BOOLEAN,\n",
    "        has_trace BOOLEAN,\n",
    "        resource_count INT,\n",
    "        source TEXT DEFAULT 'webarena_human_traces'\n",
    "    )\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "human_traj_dir = DATA_DIR / \"webarena\" / \"human_trajectories\"\n",
    "\n",
    "if human_traj_dir.exists():\n",
    "    print(f\"üîÑ Processing WebArena human trajectories from {human_traj_dir}...\")\n",
    "    \n",
    "    # Find all .trace directories (e.g., 4.trace, 7.trace, etc.)\n",
    "    trace_dirs = [d for d in human_traj_dir.iterdir() if d.is_dir() and d.name.endswith('.trace')]\n",
    "    \n",
    "    print(f\"   Found {len(trace_dirs)} .trace directories\")\n",
    "    \n",
    "    for trace_dir in sorted(trace_dirs):\n",
    "        try:\n",
    "            parsed = parse_trace_directory(trace_dir)\n",
    "            trace_id = f\"webarena_human_{trace_dir.name}\"\n",
    "            \n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT OR REPLACE INTO webarena_human_traces VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", (\n",
    "                trace_id,\n",
    "                parsed['trace_dir'],\n",
    "                parsed['files']['has_network'],\n",
    "                parsed['files']['has_stacks'],\n",
    "                parsed['files']['has_trace'],\n",
    "                parsed['resource_count'],\n",
    "                'webarena_human_traces'\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Error processing {trace_dir.name}: {e}\")\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM webarena_human_traces\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"‚úÖ Inserted {count} human trace records\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Human trajectories directory not found at {human_traj_dir}\")\n",
    "    print(f\"   Expected structure: {human_traj_dir}/4.trace, 7.trace, etc.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a2d107e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing WebArena LLM v2 trajectories from ../data/webarena/llm_trajectories_v2...\n",
      "   Found 1 model directories\n",
      "   Processing 919_gpt4_8k_cot: 810 tasks\n",
      "‚úÖ Inserted 810 LLM trajectory records\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: PARSE WEBARENA LLM TRAJECTORIES v2 (HTML render format)\n",
    "# ============================================================================\n",
    "\n",
    "def parse_merged_log(log_path: Path) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse merged_log.txt from LLM trajectories.\n",
    "    Extract: Intent, Result (PASS/FAIL) for each render_*.html\n",
    "    \n",
    "    Format:\n",
    "    2023-09-24 16:32:42,509 - INFO - [Intent]: What is the top-1 best-selling product in 2022\n",
    "    2023-09-24 16:33:07,065 - INFO - [Result] (FAIL) /path/to/0.json\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    with open(log_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    current_task_id = None\n",
    "    current_intent = None\n",
    "    \n",
    "    for line in lines:\n",
    "        # Extract intent\n",
    "        if \"[Intent]:\" in line:\n",
    "            match = re.search(r\"\\[Intent\\]:\\s*(.+?)(?:\\s|$)\", line)\n",
    "            if match:\n",
    "                current_intent = match.group(1).strip()\n",
    "        \n",
    "        # Extract result (PASS/FAIL)\n",
    "        if \"[Result]\" in line:\n",
    "            # Extract task ID from path (e.g., /tmp/.../0.json ‚Üí 0)\n",
    "            match_result = re.search(r\"\\((\\w+)\\)\", line)\n",
    "            match_task = re.search(r\"/(\\d+)\\.json\", line)\n",
    "            \n",
    "            if match_result and match_task:\n",
    "                task_id = match_task.group(1)\n",
    "                result_status = match_result.group(1)\n",
    "                \n",
    "                results[task_id] = {\n",
    "                    \"intent\": current_intent,\n",
    "                    \"result\": result_status.lower() == \"pass\"\n",
    "                }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def parse_webarena_html(html_path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse render_*.html files from WebArena LLM trajectories.\n",
    "    Extract: observations, URLs, predictions, actions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(html_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        observations = [div.get_text()[:200] for div in soup.find_all(\"div\", {\"class\": \"state_obv\"})]\n",
    "        urls = [h3.get_text() for h3 in soup.find_all(\"h3\", {\"class\": \"url\"})]\n",
    "        raw_predictions = [div.get_text()[:200] for div in soup.find_all(\"div\", {\"class\": \"raw_parsed_prediction\"})]\n",
    "        actions = [div.get_text()[:200] for div in soup.find_all(\"div\", {\"class\": \"action_object\"})]\n",
    "        \n",
    "        return {\n",
    "            \"observations\": observations,\n",
    "            \"urls\": urls,\n",
    "            \"predictions\": raw_predictions,\n",
    "            \"actions\": actions,\n",
    "            \"num_steps\": len(observations)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"num_steps\": 0}\n",
    "\n",
    "# Create table for WebArena LLM trajectories\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS webarena_llm_traces (\n",
    "        trajectory_id TEXT PRIMARY KEY,\n",
    "        model TEXT,\n",
    "        config TEXT,\n",
    "        task_id INT,\n",
    "        intent TEXT,\n",
    "        passed BOOLEAN,\n",
    "        num_steps INT,\n",
    "        observations TEXT,\n",
    "        actions TEXT,\n",
    "        urls TEXT,\n",
    "        predictions TEXT,\n",
    "        source TEXT DEFAULT 'webarena_llm_traces'\n",
    "    )\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "llm_traj_v2_dir = DATA_DIR / \"webarena\" / \"llm_trajectories_v2\"\n",
    "\n",
    "if llm_traj_v2_dir.exists():\n",
    "    print(f\"üîÑ Processing WebArena LLM v2 trajectories from {llm_traj_v2_dir}...\")\n",
    "    \n",
    "    # Find all unzipped model folders (e.g., v2_919_gpt4_8k_cot/)\n",
    "    model_dirs = [d for d in llm_traj_v2_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    print(f\"   Found {len(model_dirs)} model directories\")\n",
    "    \n",
    "    for model_dir in sorted(model_dirs):\n",
    "        # Parse model name from directory\n",
    "        # Expected format: v2_919_gpt4_8k_cot or similar\n",
    "        model_name = model_dir.name\n",
    "        \n",
    "        # Parse merged_log.txt for pass/fail + intent\n",
    "        log_file = model_dir / \"merged_log.txt\"\n",
    "        if not log_file.exists():\n",
    "            print(f\"   ‚ö†Ô∏è  No merged_log.txt in {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        log_data = parse_merged_log(log_file)\n",
    "        print(f\"   Processing {model_name}: {len(log_data)} tasks\")\n",
    "        \n",
    "        # Parse all render_*.html files\n",
    "        for html_file in sorted(model_dir.glob(\"render_*.html\")):\n",
    "            try:\n",
    "                task_id = int(html_file.stem.replace(\"render_\", \"\"))\n",
    "                parsed_html = parse_webarena_html(html_file)\n",
    "                \n",
    "                # Get intent and result from log\n",
    "                log_info = log_data.get(str(task_id), {})\n",
    "                intent = log_info.get(\"intent\", \"unknown\")\n",
    "                passed = log_info.get(\"result\", False)\n",
    "                \n",
    "                trajectory_id = f\"webarena_llm_{model_name}_task_{task_id}\"\n",
    "                \n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT OR REPLACE INTO webarena_llm_traces VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                \"\"\", (\n",
    "                    trajectory_id,\n",
    "                    model_name,\n",
    "                    \"\",  # config can be parsed from model_name if needed\n",
    "                    task_id,\n",
    "                    intent,\n",
    "                    passed,\n",
    "                    parsed_html.get(\"num_steps\", 0),\n",
    "                    json.dumps(parsed_html.get(\"observations\", [])),\n",
    "                    json.dumps(parsed_html.get(\"actions\", [])),\n",
    "                    json.dumps(parsed_html.get(\"urls\", [])),\n",
    "                    json.dumps(parsed_html.get(\"predictions\", [])),\n",
    "                    'webarena_llm_traces'\n",
    "                ))\n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ö†Ô∏è  Error processing {html_file}: {e}\")\n",
    "        \n",
    "        conn.commit()\n",
    "    \n",
    "    cursor.execute(\"SELECT COUNT(*) FROM webarena_llm_traces\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"‚úÖ Inserted {count} LLM trajectory records\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  LLM trajectories v2 directory not found at {llm_traj_v2_dir}\")\n",
    "    print(f\"   Expected structure: {llm_traj_v2_dir}/v2_919_gpt4_8k_cot/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ff74d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TAU2-BENCH found at ../data/tau2_repo\n",
      "üîÑ Processing TAU2 domain: airline\n",
      "  ‚úÖ Inserted 50 airline tasks\n",
      "üîÑ Processing TAU2 domain: retail\n",
      "  ‚úÖ Inserted 114 retail tasks\n",
      "üîÑ Processing TAU2 domain: telecom\n",
      "  ‚úÖ Inserted 2285 telecom tasks\n",
      "üîÑ Processing TAU2 domain: mock\n",
      "  ‚úÖ Inserted 9 mock tasks\n",
      "üîÑ Processing TAU2 results from ../data/tau2_repo/data/tau2/results/final\n",
      "‚úÖ Inserted 26 TAU2 result records\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: INGEST TAU2-BENCH DATASETS (FIXED MOCK DOMAIN)\n",
    "# ============================================================================\n",
    "\n",
    "# Assume tau2-bench has been cloned or downloaded manually\n",
    "tau2_repo_path = DATA_DIR / \"tau2_repo\"\n",
    "\n",
    "if not tau2_repo_path.exists():\n",
    "    print(f\"‚ö†Ô∏è  TAU2-BENCH not found at {tau2_repo_path}\")\n",
    "    print(f\"   Please clone: git clone https://github.com/sierra-research/tau2-bench.git {tau2_repo_path}\")\n",
    "else:\n",
    "    print(f\"‚úÖ TAU2-BENCH found at {tau2_repo_path}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Parse TAU2 Domains (airline, retail, telecom, mock)\n",
    "    # =========================================================================\n",
    "    \n",
    "    tau2_domains_path = tau2_repo_path / \"data\" / \"tau2\" / \"domains\"\n",
    "    \n",
    "    if tau2_domains_path.exists():\n",
    "        # UPDATED SCHEMA: Changed task_num from INT to TEXT\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS tau2_domains (\n",
    "                task_id TEXT PRIMARY KEY,\n",
    "                domain TEXT,\n",
    "                task_num TEXT, \n",
    "                reason_for_call TEXT,\n",
    "                known_info TEXT,\n",
    "                unknown_info TEXT,\n",
    "                task_instructions TEXT,\n",
    "                actions_required TEXT,\n",
    "                source TEXT DEFAULT 'tau2'\n",
    "            )\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "        \n",
    "        domains = ['airline', 'retail', 'telecom', 'mock']\n",
    "        \n",
    "        for domain in domains:\n",
    "            domain_path = tau2_domains_path / domain\n",
    "            tasks_file = domain_path / \"tasks.json\"\n",
    "            \n",
    "            if tasks_file.exists():\n",
    "                print(f\"üîÑ Processing TAU2 domain: {domain}\")\n",
    "                \n",
    "                with open(tasks_file, 'r') as f:\n",
    "                    tasks = json.load(f)\n",
    "                \n",
    "                for task in tasks:\n",
    "                    task_id = f\"tau2_{domain}_{task['id']}\"\n",
    "                    user_scenario = task.get('user_scenario', {})\n",
    "                    \n",
    "                    # FIX: 'instructions' can be a string (in mock) or dict (others)\n",
    "                    instructions_raw = user_scenario.get('instructions', {})\n",
    "                    \n",
    "                    if isinstance(instructions_raw, dict):\n",
    "                        # Standard format (Airline, Retail, Telecom)\n",
    "                        instructions = instructions_raw\n",
    "                        task_instr = instructions.get('task_instructions', '')\n",
    "                    else:\n",
    "                        # Mock format: 'instructions' is just the instruction text directly\n",
    "                        instructions = {}\n",
    "                        task_instr = str(instructions_raw) # Treat the whole string as the task instruction\n",
    "                    \n",
    "                    # FIX: Treat ID as string, do not int()\n",
    "                    original_id = str(task['id'])\n",
    "                    eval_criteria = task.get('evaluation_criteria', {})\n",
    "                    \n",
    "                    cursor.execute(\"\"\"\n",
    "                        INSERT OR REPLACE INTO tau2_domains VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                    \"\"\", (\n",
    "                        task_id,\n",
    "                        domain,\n",
    "                        original_id,\n",
    "                        instructions.get('reason_for_call', ''),\n",
    "                        instructions.get('known_info', ''),\n",
    "                        instructions.get('unknown_info', ''),\n",
    "                        task_instr, # Uses the string directly if mock\n",
    "                        json.dumps(eval_criteria.get('actions', []))[:1000],\n",
    "                        'tau2'\n",
    "                    ))\n",
    "                \n",
    "                conn.commit()\n",
    "                print(f\"  ‚úÖ Inserted {len(tasks)} {domain} tasks\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  {tasks_file} not found\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  TAU2 domains path not found: {tau2_domains_path}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Parse TAU2 Results\n",
    "    # =========================================================================\n",
    "    \n",
    "    tau2_results_path = tau2_repo_path / \"data\" / \"tau2\" / \"results\" / \"final\"\n",
    "    \n",
    "    if tau2_results_path.exists():\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS tau2_results (\n",
    "                result_id TEXT PRIMARY KEY,\n",
    "                model TEXT,\n",
    "                domain TEXT,\n",
    "                num_trials INT,\n",
    "                result_json TEXT,\n",
    "                source TEXT DEFAULT 'tau2_results'\n",
    "            )\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "        \n",
    "        print(f\"üîÑ Processing TAU2 results from {tau2_results_path}\")\n",
    "        \n",
    "        for result_file in tau2_results_path.glob(\"*.json\"):\n",
    "            try:\n",
    "                with open(result_file, 'r') as f:\n",
    "                    result_data = json.load(f)\n",
    "                \n",
    "                result_id = result_file.stem\n",
    "                parts = result_file.stem.split('_')\n",
    "                model = parts[0] if parts else 'unknown'\n",
    "                domain = parts[1] if len(parts) > 1 else 'unknown'\n",
    "                \n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT OR REPLACE INTO tau2_results VALUES (?, ?, ?, ?, ?, ?)\n",
    "                \"\"\", (\n",
    "                    result_id,\n",
    "                    model,\n",
    "                    domain,\n",
    "                    len(result_data) if isinstance(result_data, list) else 1,\n",
    "                    json.dumps(result_data)[:5000],\n",
    "                    'tau2_results'\n",
    "                ))\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Error processing {result_file}: {e}\")\n",
    "        \n",
    "        conn.commit()\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM tau2_results\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"‚úÖ Inserted {count} TAU2 result records\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  TAU2 results path not found: {tau2_results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8d7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from database: ../data/db/unified.db\n",
      "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
      "‚ïë              UNIFIED DATABASE SUMMARY                        ‚ïë\n",
      "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
      "\n",
      "üìä Data Ingestion Complete:\n",
      "   WebLINX samples:              1,000\n",
      "   WebArena tasks:               812\n",
      "   WebArena human traces:        2\n",
      "   WebArena LLM traces:          810\n",
      "   TAU2 domain tasks:            2,458\n",
      "   TAU2 result logs:             26\n",
      "\n",
      "   TOTAL RECORDS:                5,108\n",
      "\n",
      "‚úÖ Summary saved to ../data/processed/dataset_summary.json\n",
      "\n",
      "üìã Tables in database: ['weblinx', 'webarena_tasks', 'webarena_human_traces', 'webarena_llm_traces', 'tau2_domains', 'tau2_results']\n",
      "‚úÖ Database closed: ../data/db/unified.db\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: CREATE UNIFIED DATABASE VIEWS & SUMMARY (FIXED)\n",
    "# ============================================================================\n",
    "\n",
    "# Re-connect to ensure we are reading the actual persistent database file\n",
    "# This fixes \"Cannot operate on a closed database\" and ensures we see committed data\n",
    "if 'conn' in locals():\n",
    "    try:\n",
    "        conn.close()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "summary_stats = {}\n",
    "\n",
    "# Get counts for all tables\n",
    "tables = [\n",
    "    'weblinx',\n",
    "    'webarena_tasks',\n",
    "    'webarena_human_traces',\n",
    "    'webarena_llm_traces',\n",
    "    'tau2_domains',\n",
    "    'tau2_results'\n",
    "]\n",
    "\n",
    "print(f\"Reading from database: {DB_PATH}\")\n",
    "\n",
    "for table in tables:\n",
    "    try:\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        summary_stats[f'{table}_count'] = count\n",
    "    except sqlite3.OperationalError:\n",
    "        # Table doesn't exist\n",
    "        summary_stats[f'{table}_count'] = 0\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {table}: {e}\")\n",
    "        summary_stats[f'{table}_count'] = 0\n",
    "\n",
    "# Print summary\n",
    "print(\"‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\")\n",
    "print(\"‚ïë              UNIFIED DATABASE SUMMARY                        ‚ïë\")\n",
    "print(\"‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\")\n",
    "print(f\"\\nüìä Data Ingestion Complete:\")\n",
    "print(f\"   WebLINX samples:              {summary_stats.get('weblinx_count', 0):,}\")\n",
    "print(f\"   WebArena tasks:               {summary_stats.get('webarena_tasks_count', 0):,}\")\n",
    "print(f\"   WebArena human traces:        {summary_stats.get('webarena_human_traces_count', 0):,}\")\n",
    "print(f\"   WebArena LLM traces:          {summary_stats.get('webarena_llm_traces_count', 0):,}\")\n",
    "print(f\"   TAU2 domain tasks:            {summary_stats.get('tau2_domains_count', 0):,}\")\n",
    "print(f\"   TAU2 result logs:             {summary_stats.get('tau2_results_count', 0):,}\")\n",
    "\n",
    "total_samples = sum(summary_stats.values())\n",
    "print(f\"\\n   TOTAL RECORDS:                {total_samples:,}\")\n",
    "\n",
    "# Save summary\n",
    "summary_path = PROCESSED_DIR / \"dataset_summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump({\n",
    "        **summary_stats,\n",
    "        \"ingestion_date\": datetime.now().isoformat(),\n",
    "        \"database_path\": str(DB_PATH),\n",
    "        \"tables\": tables\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Summary saved to {summary_path}\")\n",
    "\n",
    "# List all tables in database\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "all_tables = cursor.fetchall()\n",
    "print(f\"\\nüìã Tables in database: {[t[0] for t in all_tables]}\")\n",
    "\n",
    "# Close database\n",
    "conn.close()\n",
    "print(f\"‚úÖ Database closed: {DB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d5c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: INGEST FUJITSU AGENTIC RAG RED TEAMING DATASET\n",
    "# ============================================================================\n",
    "\n",
    "fujitsu_dir = DATA_DIR / \"fujitsu\"\n",
    "fujitsu_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the expected files based on the dataset readme\n",
    "fujitsu_files = {\n",
    "    \"orchestrator\": \"orchestrator_attacks_combined_deduplicated.jsonl\",\n",
    "    \"rag_poison\": \"rag_poisoning_benchmark_combined_deduplicated.jsonl\",\n",
    "    \"direct_query\": \"safety_benchmark_direct_query_combined_deduplicated.jsonl\",\n",
    "    \"image_poison\": \"image_poisoning_simulation_results_20250504_202954.jsonl\"\n",
    "}\n",
    "\n",
    "# 1. Create Table\n",
    "# We use a flexible schema because columns vary significantly between B1, B2, B3, B4\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS fujitsu_red_teaming (\n",
    "        unique_id TEXT PRIMARY KEY,\n",
    "        original_id TEXT,\n",
    "        dataset_subset TEXT,  -- orchestrator, rag_poison, etc.\n",
    "        adversarial_goal TEXT,\n",
    "        user_query TEXT,      -- The actual input sent to the system\n",
    "        injection_payload TEXT, -- Specific payload if separated (e.g., hidden JSON)\n",
    "        target_output TEXT,   -- The LLM/System response\n",
    "        success BOOLEAN,      -- Whether the attack succeeded\n",
    "        full_json TEXT,       -- Store all specific metadata here\n",
    "        source TEXT DEFAULT 'fujitsu'\n",
    "    )\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "print(f\"üîÑ Processing Fujitsu Red Teaming data from {fujitsu_dir}...\")\n",
    "\n",
    "total_fujitsu_records = 0\n",
    "\n",
    "for subset_name, filename in fujitsu_files.items():\n",
    "    file_path = fujitsu_dir / filename\n",
    "    \n",
    "    if file_path.exists():\n",
    "        print(f\"   Processing {subset_name}...\")\n",
    "        records_added = 0\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    \n",
    "                    # --- NORMALIZE FIELDS BASED ON SUBSET ---\n",
    "                    # Different files use different keys (id vs attack_id, etc.)\n",
    "                    \n",
    "                    # 1. ID Handling\n",
    "                    orig_id = record.get('id') or record.get('attack_id') or record.get('record_id')\n",
    "                    unique_id = f\"fujitsu_{subset_name}_{orig_id}\"\n",
    "                    \n",
    "                    # 2. Query/Payload Handling\n",
    "                    user_query = \"\"\n",
    "                    injection_payload = \"\"\n",
    "                    goal = record.get('adversarial_goal', '')\n",
    "                    output = \"\"\n",
    "                    \n",
    "                    if subset_name == \"orchestrator\":\n",
    "                        # B4: Has benign_query + malicious_injection -> combined_query\n",
    "                        user_query = record.get('combined_query', '')\n",
    "                        injection_payload = record.get('malicious_injection', '')\n",
    "                        output = record.get('simulated_tool', '') # The tool it flipped to\n",
    "                        \n",
    "                    elif subset_name == \"rag_poison\":\n",
    "                        # B1: Poison content is retrieved, user_query triggers it\n",
    "                        user_query = record.get('user_query', '')\n",
    "                        injection_payload = record.get('poison_content', '')\n",
    "                        output = record.get('target_llm_output', '')\n",
    "                        \n",
    "                    elif subset_name == \"direct_query\":\n",
    "                        # B3: Direct prompt injection\n",
    "                        user_query = record.get('user_query', '')\n",
    "                        injection_payload = record.get('adversarial_suffix', '') # Sometimes suffix, sometimes embedded\n",
    "                        output = record.get('target_llm_output', '')\n",
    "                        \n",
    "                    elif subset_name == \"image_poison\":\n",
    "                        # B2: Payload is in the image\n",
    "                        user_query = record.get('user_query', '')\n",
    "                        injection_payload = record.get('poison_payload', '')\n",
    "                        output = record.get('mta_output', '')\n",
    "\n",
    "                    # 3. Success Handling (Usually boolean or string judgment)\n",
    "                    success_raw = record.get('success') or record.get('mta_rag_success')\n",
    "                    # Some files might not have explicit boolean success in all lines, default to True as this is a \"successful attacks\" corpus\n",
    "                    is_success = True if success_raw is None else bool(success_raw)\n",
    "\n",
    "                    # Insert\n",
    "                    cursor.execute(\"\"\"\n",
    "                        INSERT OR REPLACE INTO fujitsu_red_teaming VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                    \"\"\", (\n",
    "                        unique_id,\n",
    "                        str(orig_id),\n",
    "                        subset_name,\n",
    "                        goal,\n",
    "                        user_query,\n",
    "                        injection_payload,\n",
    "                        output,\n",
    "                        is_success,\n",
    "                        json.dumps(record),\n",
    "                        'fujitsu'\n",
    "                    ))\n",
    "                    records_added += 1\n",
    "                    \n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    # simplistic error skipping to keep ingestion moving\n",
    "                    continue\n",
    "        \n",
    "        print(f\"     ‚úì Added {records_added} records from {subset_name}\")\n",
    "        total_fujitsu_records += records_added\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è File not found: {filename} (Skipping)\")\n",
    "\n",
    "conn.commit()\n",
    "print(f\"‚úÖ Fujitsu ingestion complete. Total records: {total_fujitsu_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cceffe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing database queries...\n",
      "\n",
      "1Ô∏è‚É£  WebLINX sample:\n",
      "                  id  demo_id  turn_id                           action                                                                                                                                                                                                                                                                                  action_history       utterances                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       candidates                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      clean_html      viewport   source\n",
      "0  weblinx_apfyesq_6  apfyesq        6  click(uid=\"67e2a5fb-8b1d-41a0\")  say(speaker=\"navigator\", utterance=\"Hi\")</s><s>[INST] say(speaker=\"instructor\", utterance=\"Open Encyclopedia website.\") say(speaker=\"navigator\", utterance=\"Yes, sure\") load(url=\"https://www.encyclopedia.com/\")</s><s>[INST] say(speaker=\"instructor\", utterance=\"Search for biotechnology\")  [00:05] Hello ;  (uid = 67e2a5fb-8b1d-41a0) [[tag]] input [[xpath]] /html/body/div[2...form/div[1]/input [[bbox]] x=419.6 y=461.0 width=477.6 height=89.6 [[attributes]] title='' value='' name='...What do you want to learn today?' \\n(uid = fedfb512-949e-42b3) [[tag]] input [[xpath]] /html/body/div[2...form/div[2]/input [[bbox]] x=915.6 y=461.0 width=185.6 height=89.6 [[attributes]] type='submit' value='Search...-form-submit form-submit' \\n(uid = c7fbc11c-0949-4ab2) [[tag]] form [[xpath]] /html/body/div[2...2]/div[3]/form [[bbox]] x=419.6 y=461.0 width=680 height=88 [[attributes]] method='get' data-web...clopedia.com/gsearch' [[children]] div div \\n(uid = 6c7fe1f1-f640-4dce) [[tag]] span [[xpath]] /html/body/header/div...div[2]/a/span [[text]] EXPLORE [[bbox]] x=1240.5 y=28.6 width=54.1 height=30 [[attributes]] class='text' data-webtasks...-main-menu-menu' \\n(uid = 0ffc6f0e-808a-4c2a) [[tag]] span [[xpath]] /html/body/div[5]/span [[text]] √ó [[bbox]] x=1485.9 y=665.6 width=23.3 height=21.6 [[attributes]] class='adthrive-close...8a-4c2a' \\n(uid = 8d8afc84-5b97-477a) [[tag]] div [[xpath]] /html/body/div[...3]/form/div[1] [[text]]   [[bbox]] x=419.6 y=461.0 width=476 height=88 [[attributes]] data-webtasks-id='8...keys form-no-label' [[children]] span input \\n(uid = 1ea51e98-3fcd-4e30) [[tag]] h4 [[xpath]] /html/body/div[...div/div[1]/h4 [[text]] The World‚Äôs #1 Online Encyclopedia [[bbox]] x=33 y=163 width=1453.2 height=43.2 [[attributes]] data-webtasks-id='1...cd-4e30' \\n(uid = 769785af-485e-4cf1) [[tag]] a [[xpath]] /html/body/header/div...2]/div[2]/a [[bbox]] x=1240.5 y=28.6 width=74.1 height=30 [[attributes]] id='rcLink' class='... false; toggleFlyout()' [[children]] span i \\n(uid = e7b7879f-45ae-48a5) [[tag]] i [[xpath]] /html/body/header/div...div[2]/a/i [[bbox]] x=1294.6 y=33.6 width=20 height=20 [[attributes]] class='fa ency-down...5ae-48a5' \\n(uid = bf33a062-fb67-44f0) [[tag]] a [[xpath]] /html/body/div[2...div[4]/p/a [[text]] Read more [[bbox]] x=567.0 y=641.0 width=69.3 height=16 [[attributes]] href='/about' data-...67-44f0'  (html(body(div class=\"container\"(div class=\"row\"(div class=\"col hdr-r justify-...flex align-items-center\"(div class=\"hdr-categories-container\"(a class=\"rc-link\" onclick=\"if (!window.__cfRLUn... false; toggleFlyout()\" data-webtasks-id=\"76978...85e-4cf1\"(span class=\"text\" data-webtasks-id=\"6c7fe1...640-4dce\"EXPLORE)(i class=\"fa ency-down\" data-webtasks-id=\"e7b787...5ae-48a5\"))(div class=\"rc-flyout\"))))) (div (div class=\"dialog-off-canvas-main-canvas\"(div class=\"homepage\"(div style=\"background-image: url('/sites...01_3.png');\" class=\"ency-loaded\"(div class=\"ency-loaded mask-hero\")(h4 data-webtasks-id=\"1ea51e...fcd-4e30\"The World‚Äôs #1 Online Encyclopedia)(div class=\"clear-both hero\"(div class=\"ency-hero-search\"(form action=\"https://www.encyclopedia.com/gsearch\" method=\"get\" data-webtasks-id=\"c7fbc11c...49-4ab2\"(div class=\"js-form-item form-...-keys form-no-label\" data-webtasks-id=\"8d8afc8...7-477a\" (span class=\"field-preffix\" (input class=\"button js-form-submit form-submit\" type=\"submit\" value=\"\" ) (input title=\"\" class=\"searchbox form-search form-input\" placeholder=\"What do you want to learn today?\" type=\"search\" name=\"q\" value=\"\" size=\"15\" maxlength=\"128\" data-webtasks-id=\"67e2a5...d-41a0\" spellcheck=\"false\" (span class=\"field-suffix\" (i class=\"fa ency-close\")))(div class=\"form-actions js-form-wrapper form-wrapper\" (input class=\"button js-form-submit form-submit\" type=\"submit\" value=\"Search\" data-webtasks-id=\"fedfb512-...9e-42b3\")))(div class=\"clear-both hero footer-copy\"(a href=\"/about\" data-webtasks-id=\"bf33a0...67-44f0\"Read more) about our content and why so many people love it.))))))(div class=\"adthrive-ad adth...cls adthrive-sticky\" style=\"min-height: 90px;\" closable=\"true\"(div style=\"border: 0pt none;\")(span class=\"adthrive-close\" data-webtasks-id=\"0ffc6f0...8a-4c2a\"√ó))))  746h x 1536w  weblinx\n",
      "\n",
      "2Ô∏è‚É£  WebArena tasks sample:\n",
      "   task_id               sites  require_login           start_url                                          intent                                         intent_template        eval_types                             reference_answers    source\n",
      "0        0  ['shopping_admin']              1  __SHOPPING_ADMIN__  What is the top-1 best-selling product in 2022  What is the top-{{n}} best-selling product in {{year}}  ['string_match']  {\"exact_match\": \"Quest Lumaflex\\u2122 Band\"}  webarena\n",
      "\n",
      "3Ô∏è‚É£  WebArena LLM traces sample:\n",
      "                         trajectory_id            model config  task_id intent  passed  num_steps                                                                                                                                                                                                           observations                                                                                                                                                                                                       actions                                                              urls                                                                                                                                                                                                      predictions               source\n",
      "0  webarena_llm_919_gpt4_8k_cot_task_0  919_gpt4_8k_cot               0   What       0          1  [\"Tab 0 (current): Dashboard / Magento Admin\\n\\n[1] RootWebArea 'Dashboard / Magento Admin' focused: True\\n\\t[1306] link 'Magento Admin Panel'\\n\\t\\t[1307] img 'Magento Admin Panel'\\n\\t[1218] menubar '' orientati\"]  [\"{'action_type': , 'coords': array([0., 0.], dtype=float32), 'element_role': 0, 'element_name': '', 'text': [], 'page_number': 0, 'url': '', 'nth': 0, 'pw_code': '', 'element_id': '', 'key_comb': '', '\"]  [\"URL: http://metis.lti.cs.cmu.edu:7780/admin/admin/dashboard/\"]  [\"Let's think step-by-step. The information on the top-selling products is under the \\\"Bestsellers\\\" tab, which is currently expanded. The top-1 best-selling product is listed as \\\"Sprite Stasis Ball 65 cm\"]  webarena_llm_traces\n",
      "\n",
      "4Ô∏è‚É£  TAU2 retail tasks sample:\n",
      "         task_id  domain  task_num                                                                                                                                                                                                                                                                                                reason_for_call                              known_info                             unknown_info                                                                 task_instructions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     actions_required source\n",
      "0  tau2_retail_0  retail         0  You received your order #W2378156 and wish to exchange the mechanical keyboard for a similar one but with clicky switches and the smart thermostat for one compatible with Google Home instead of Apple HomeKit. If there is no keyboard that is clicky, RGB backlight, full size, you'd go for no backlight.  You are Yusuf Rossi in zip code 19122.  You do not remember your email address.  You are detail-oriented and want to make sure everything is addressed in one go.  [{\"action_id\": \"0_0\", \"name\": \"find_user_id_by_name_zip\", \"arguments\": {\"first_name\": \"Yusuf\", \"last_name\": \"Rossi\", \"zip\": \"19122\"}, \"info\": null}, {\"action_id\": \"0_1\", \"name\": \"get_order_details\", \"arguments\": {\"order_id\": \"#W2378156\"}, \"info\": null}, {\"action_id\": \"0_2\", \"name\": \"get_product_details\", \"arguments\": {\"product_id\": \"1656367028\"}, \"info\": null}, {\"action_id\": \"0_3\", \"name\": \"get_product_details\", \"arguments\": {\"product_id\": \"4896585277\"}, \"info\": null}, {\"action_id\": \"0_4\", \"name\": \"exchange_delivered_order_items\", \"arguments\": {\"order_id\": \"#W2378156\", \"item_ids\": [\"1151293680\", \"4983901480\"], \"new_item_ids\": [\"7706410293\", \"7747408585\"], \"payment_method_id\": \"credit_card_9513926\"}, \"info\": null}]   tau2\n",
      "\n",
      "‚úÖ Utility functions ready for use\n",
      "\n",
      "5Ô∏è‚É£  Fujitsu Orchestrator sample:\n",
      "  dataset_subset adversarial_goal  success\n",
      "0   orchestrator                         1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: UTILITY FUNCTIONS FOR QUERYING & EXPORTING\n",
    "# ============================================================================\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Get connection to unified database\"\"\"\n",
    "    return sqlite3.connect(DB_PATH)\n",
    "\n",
    "def query_weblinx(limit: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Query WebLINX samples\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM weblinx LIMIT {limit}\", conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def query_webarena_tasks(limit: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Query WebArena tasks\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM webarena_tasks LIMIT {limit}\", conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def query_webarena_llm(model: Optional[str] = None, limit: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Query WebArena LLM trajectories\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    if model:\n",
    "        query = f\"SELECT * FROM webarena_llm_traces WHERE model LIKE '%{model}%' LIMIT {limit}\"\n",
    "    else:\n",
    "        query = f\"SELECT * FROM webarena_llm_traces LIMIT {limit}\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def query_tau2(domain: str, limit: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Query TAU2 tasks by domain\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    df = pd.read_sql_query(\n",
    "        f\"SELECT * FROM tau2_domains WHERE domain = '{domain}' LIMIT {limit}\",\n",
    "        conn\n",
    "    )\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def export_to_json(table_name: str, output_path: Path) -> None:\n",
    "    \"\"\"Export entire table to JSON\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
    "    conn.close()\n",
    "    df.to_json(output_path, orient='records', indent=2)\n",
    "    print(f\"‚úÖ Exported {len(df)} records to {output_path}\")\n",
    "\n",
    "# Test queries\n",
    "print(\"Testing database queries...\\n\")\n",
    "\n",
    "print(\"1Ô∏è‚É£  WebLINX sample:\")\n",
    "print(query_weblinx(limit=1).to_string())\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  WebArena tasks sample:\")\n",
    "try:\n",
    "    print(query_webarena_tasks(limit=1).to_string())\n",
    "except Exception as e:\n",
    "    print(f\"   (No data: {e})\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£  WebArena LLM traces sample:\")\n",
    "try:\n",
    "    print(query_webarena_llm(limit=1).to_string())\n",
    "except Exception as e:\n",
    "    print(f\"   (No data: {e})\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£  TAU2 retail tasks sample:\")\n",
    "try:\n",
    "    print(query_tau2(domain='retail', limit=1).to_string())\n",
    "except Exception as e:\n",
    "    print(f\"   (No data: {e})\")\n",
    "\n",
    "print(\"\\n‚úÖ Utility functions ready for use\")\n",
    "def query_fujitsu(subset: Optional[str] = None, limit: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Query Fujitsu Red Teaming samples\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    if subset:\n",
    "        query = f\"SELECT * FROM fujitsu_red_teaming WHERE dataset_subset = '{subset}' LIMIT {limit}\"\n",
    "    else:\n",
    "        query = f\"SELECT * FROM fujitsu_red_teaming LIMIT {limit}\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "# Test the query\n",
    "try:\n",
    "    print(\"\\n5Ô∏è‚É£  Fujitsu Orchestrator sample:\")\n",
    "    print(query_fujitsu(subset='orchestrator', limit=1)[['dataset_subset', 'adversarial_goal', 'success']].to_string())\n",
    "except Exception as e:\n",
    "    print(f\"   (No data: {e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b34d5d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing AgentDojo data from ../data/agent_dojo...\n",
      "   Processing agentdojo-claude-3-5-sonnet-20241022.jsonl...\n",
      "     ‚úì Added 194 records\n",
      "‚úÖ AgentDojo ingestion complete. Total records: 194\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: INGEST AGENT DOJO DATASET\n",
    "# ============================================================================\n",
    "\n",
    "agent_dojo_dir = DATA_DIR / \"agent_dojo\"\n",
    "agent_dojo_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create Table for AgentDojo\n",
    "# We store the full conversation structure as it is rich with tool calls and 'thinking' tags\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS agent_dojo (\n",
    "        unique_id TEXT PRIMARY KEY,\n",
    "        suite_name TEXT,\n",
    "        user_task_id TEXT,\n",
    "        injection_task_id TEXT, -- If NULL, this is a benign task\n",
    "        is_attack BOOLEAN,      -- Derived: True if injection_task_id is not None\n",
    "        success BOOLEAN,        -- Did the agent complete the user task?\n",
    "        security BOOLEAN,       -- Did the agent prevent the attack? (True = Secure, False = Compromised)\n",
    "        prompt TEXT,            -- The main user instruction\n",
    "        thinking_trace TEXT,    -- Extracted <thinking> blocks (CoT)\n",
    "        full_conversation TEXT, -- Full JSON dump of messages\n",
    "        source TEXT DEFAULT 'agent_dojo'\n",
    "    )\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "print(f\"üîÑ Processing AgentDojo data from {agent_dojo_dir}...\")\n",
    "\n",
    "dojo_files = list(agent_dojo_dir.glob(\"*.jsonl\"))\n",
    "total_dojo_records = 0\n",
    "\n",
    "for file_path in dojo_files:\n",
    "    print(f\"   Processing {file_path.name}...\")\n",
    "    file_records = 0\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                \n",
    "                # Skip file headers if present (lines that are just metadata summaries)\n",
    "                if \"messages\" not in record:\n",
    "                    continue\n",
    "                \n",
    "                meta = record.get(\"metadata\", {})\n",
    "                \n",
    "                # 1. IDs and Labels\n",
    "                # Format: user_task_0 (benign) or user_task_0_injection_task_2 (attack)\n",
    "                u_id = meta.get(\"user_task_id\", \"unknown\")\n",
    "                i_id = meta.get(\"injection_task_id\") # None if benign\n",
    "                \n",
    "                # Unique DB ID\n",
    "                suite = meta.get(\"suite_name\", \"general\")\n",
    "                unique_key = f\"dojo_{suite}_{u_id}\"\n",
    "                if i_id:\n",
    "                    unique_key += f\"_{i_id}\"\n",
    "                \n",
    "                is_attack = i_id is not None\n",
    "                \n",
    "                # 2. Extract Prompt (First User Message)\n",
    "                messages = record.get(\"messages\", [])\n",
    "                prompt = \"\"\n",
    "                for m in messages:\n",
    "                    if m['role'] == 'user':\n",
    "                        prompt = m['content']\n",
    "                        break\n",
    "                \n",
    "                # 3. Extract Chain of Thought (<thinking>)\n",
    "                # AgentDojo models often output <thinking> tags before tool calls\n",
    "                thinking_steps = []\n",
    "                for m in messages:\n",
    "                    if m['role'] == 'assistant' and isinstance(m.get('content'), str):\n",
    "                        content = m['content']\n",
    "                        if \"<thinking>\" in content:\n",
    "                            # Simple extraction between tags\n",
    "                            start = content.find(\"<thinking>\") + len(\"<thinking>\")\n",
    "                            end = content.find(\"</thinking>\")\n",
    "                            if start != -1 and end != -1:\n",
    "                                thinking_steps.append(content[start:end].strip())\n",
    "                \n",
    "                thinking_str = \"\\n---\\n\".join(thinking_steps)\n",
    "\n",
    "                # 4. Insert\n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT OR REPLACE INTO agent_dojo VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                \"\"\", (\n",
    "                    unique_key,\n",
    "                    suite,\n",
    "                    u_id,\n",
    "                    str(i_id) if i_id else None,\n",
    "                    is_attack,\n",
    "                    meta.get(\"success\", False),\n",
    "                    meta.get(\"security\", True), # Default to secure if not specified\n",
    "                    prompt,\n",
    "                    thinking_str,\n",
    "                    json.dumps(messages),\n",
    "                    'agent_dojo'\n",
    "                ))\n",
    "                \n",
    "                file_records += 1\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                # print(f\"Error on line: {e}\") \n",
    "                continue\n",
    "                \n",
    "    print(f\"     ‚úì Added {file_records} records\")\n",
    "    total_dojo_records += file_records\n",
    "\n",
    "conn.commit()\n",
    "print(f\"‚úÖ AgentDojo ingestion complete. Total records: {total_dojo_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "818cce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading AttackQA dataset from ../data/attackqa/attackqa.parquet...\n",
      "   Found 25335 records in local parquet file\n",
      "     Processed 5000 rows...\n",
      "     Processed 10000 rows...\n",
      "     Processed 15000 rows...\n",
      "     Processed 20000 rows...\n",
      "     Processed 25000 rows...\n",
      "‚úÖ AttackQA ingestion complete. Total records: 25335\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: INGEST ATTACKQA DATASET (LOCAL PARQUET)\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd # Ensure pandas is imported\n",
    "\n",
    "# Define the local path (adjusted relative to your notebook location)\n",
    "attackqa_path = DATA_DIR / \"attackqa\" / \"attackqa.parquet\"\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS attack_qa (\n",
    "        unique_id TEXT PRIMARY KEY,\n",
    "        mitre_technique_id TEXT,\n",
    "        question TEXT,\n",
    "        thought_trace TEXT,   -- The CoT reasoning\n",
    "        answer TEXT,          -- The final answer\n",
    "        context_document TEXT, -- The retrieval snippet (ground truth)\n",
    "        is_human_question BOOLEAN,\n",
    "        is_human_answer BOOLEAN,\n",
    "        source_relation TEXT, -- e.g. relationships_detects\n",
    "        full_json TEXT,\n",
    "        source TEXT DEFAULT 'attack_qa'\n",
    "    )\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "print(f\"üîÑ Loading AttackQA dataset from {attackqa_path}...\")\n",
    "\n",
    "if attackqa_path.exists():\n",
    "    try:\n",
    "        # Load directly from local parquet file\n",
    "        df = pd.read_parquet(attackqa_path)\n",
    "        \n",
    "        print(f\"   Found {len(df)} records in local parquet file\")\n",
    "        \n",
    "        records_added = 0\n",
    "        \n",
    "        # Iterate over DataFrame rows\n",
    "        for idx, row in df.iterrows():\n",
    "            try:\n",
    "                # Create unique ID\n",
    "                tech_id = row.get('subject_id') or 'unknown'\n",
    "                unique_id = f\"attackqa_{tech_id}_{idx}\"\n",
    "                \n",
    "                # Extract fields with safe defaults\n",
    "                q = row.get('question', '')\n",
    "                a = row.get('answer', '')\n",
    "                t = row.get('thought', '') # Use empty string if missing\n",
    "                doc = row.get('document', '')\n",
    "                \n",
    "                # Metadata (handle potential numpy bools by casting)\n",
    "                human_q = bool(row.get('human_question', False))\n",
    "                human_a = bool(row.get('human_answer', False))\n",
    "                \n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT OR REPLACE INTO attack_qa VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                \"\"\", (\n",
    "                    unique_id,\n",
    "                    str(tech_id),\n",
    "                    str(q),\n",
    "                    str(t) if t else \"\",\n",
    "                    str(a),\n",
    "                    str(doc),\n",
    "                    human_q,\n",
    "                    human_a,\n",
    "                    str(row.get('source', '')),\n",
    "                    row.to_json(), # Store full row as JSON\n",
    "                    'attack_qa'\n",
    "                ))\n",
    "                \n",
    "                records_added += 1\n",
    "                if (idx + 1) % 5000 == 0:\n",
    "                    print(f\"     Processed {idx + 1} rows...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # print(f\"Error on row {idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"‚úÖ AttackQA ingestion complete. Total records: {records_added}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading local parquet file: {e}\")\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {attackqa_path}\")\n",
    "    print(\"   Please ensure the parquet file is at ../data/attackqa/attackqa.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: INGEST AGENTHARM BENCHMARK\n",
    "# ============================================================================\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "agent_harm_dir = DATA_DIR / \"agent_harm\"\n",
    "agent_harm_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the files from the 'benchmark' folder in the repo\n",
    "agent_harm_files = [\n",
    "    \"benign_behaviors_test_public.json\",\n",
    "    \"benign_behaviors_validation.json\",\n",
    "    \"harmful_behaviors_test_public.json\",\n",
    "    \"harmful_behaviors_validation.json\",\n",
    "    # Chat files are smaller aux files, but we include them\n",
    "    \"chat_public_test.json\",\n",
    "    \"chat_validation.json\"\n",
    "]\n",
    "\n",
    "print(f\"‚¨áÔ∏è Downloading AgentHarm benchmark files to {agent_harm_dir}...\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS agent_harm (\n",
    "        unique_id TEXT PRIMARY KEY,\n",
    "        subset TEXT,          -- benign or harmful\n",
    "        behavior_id TEXT,\n",
    "        prompt TEXT,          -- The user instruction\n",
    "        description TEXT,     -- Detailed task description\n",
    "        target_tools TEXT,    -- Tools available/required\n",
    "        source TEXT DEFAULT 'agent_harm'\n",
    "    )\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# Download and Ingest\n",
    "total_harm_records = 0\n",
    "\n",
    "for filename in agent_harm_files:\n",
    "    try:\n",
    "        # Download\n",
    "        local_path = hf_hub_download(\n",
    "            repo_id=\"ai-safety-institute/AgentHarm\",\n",
    "            filename=f\"benchmark/{filename}\",\n",
    "            repo_type=\"dataset\",\n",
    "            local_dir=agent_harm_dir,\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "        \n",
    "        # Move out of nested 'benchmark' folder if created\n",
    "        downloaded = Path(local_path)\n",
    "        final_path = agent_harm_dir / filename\n",
    "        if downloaded.name == filename and downloaded.parent.name == \"benchmark\":\n",
    "            downloaded.rename(final_path)\n",
    "            # Try to remove empty benchmark dir\n",
    "            try: downloaded.parent.rmdir() \n",
    "            except: pass\n",
    "        \n",
    "        # Load JSON\n",
    "        # Structure: {\"behaviors\": [ ... ]}\n",
    "        if final_path.exists():\n",
    "            with open(final_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Determine subset from filename\n",
    "            subset = \"benign\" if \"benign\" in filename else \"harmful\"\n",
    "            if \"chat\" in filename: subset = \"chat_\" + subset\n",
    "            \n",
    "            # Extract list\n",
    "            items = data.get('behaviors', []) if 'behaviors' in data else data\n",
    "            \n",
    "            for item in items:\n",
    "                # Handle different schemas if chat files differ\n",
    "                b_id = item.get('id', 'unknown')\n",
    "                unique_id = f\"agentharm_{subset}_{b_id}\"\n",
    "                prompt = item.get('prompt', '') or item.get('behavior', '')\n",
    "                \n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT OR REPLACE INTO agent_harm VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                \"\"\", (\n",
    "                    unique_id,\n",
    "                    subset,\n",
    "                    str(b_id),\n",
    "                    prompt,\n",
    "                    item.get('description', ''),\n",
    "                    json.dumps(item.get('tools', [])),\n",
    "                    'agent_harm'\n",
    "                ))\n",
    "                total_harm_records += 1\n",
    "            \n",
    "            print(f\"   ‚úì Processed {filename} ({len(items)} records)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error processing {filename}: {e}\")\n",
    "\n",
    "conn.commit()\n",
    "print(f\"‚úÖ AgentHarm ingestion complete. Total records: {total_harm_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4384c47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Checking downloaded file structure...\n",
      "\n",
      "‚úÖ WebLinX config\n",
      "‚úÖ WebArena config\n",
      "‚úÖ WebArena human traces\n",
      "‚úÖ WebArena LLM v2\n",
      "‚úÖ TAU2 repo\n",
      "\n",
      "üìä File Counts:\n",
      "   Human trace directories: 2\n",
      "   LLM model directories: 1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BONUS: QUICK FILE VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üìÅ Checking downloaded file structure...\\n\")\n",
    "\n",
    "checks = {\n",
    "    \"WebLinX config\": (DATA_DIR / \"weblinx\" / \"templates\").exists(),\n",
    "    \"WebArena config\": (DATA_DIR / \"webarena\" / \"config_files\" / \"test.raw.json\").exists(),\n",
    "    \"WebArena human traces\": (DATA_DIR / \"webarena\" / \"human_trajectories\").exists() and len(list((DATA_DIR / \"webarena\" / \"human_trajectories\").glob(\"*.trace\"))) > 0,\n",
    "    \"WebArena LLM v2\": (DATA_DIR / \"webarena\" / \"llm_trajectories_v2\").exists() and len(list((DATA_DIR / \"webarena\" / \"llm_trajectories_v2\").glob(\"*/\"))) > 0,\n",
    "    \"TAU2 repo\": (DATA_DIR / \"tau2_repo\" / \"data\" / \"tau2\" / \"domains\").exists(),\n",
    "}\n",
    "\n",
    "for check_name, exists in checks.items():\n",
    "    status = \"‚úÖ\" if exists else \"‚ö†Ô∏è \"\n",
    "    print(f\"{status} {check_name}\")\n",
    "\n",
    "# Count files\n",
    "human_traces = list((DATA_DIR / \"webarena\" / \"human_trajectories\").glob(\"*.trace\"))\n",
    "llm_models = list((DATA_DIR / \"webarena\" / \"llm_trajectories_v2\").glob(\"*/\"))\n",
    "\n",
    "print(f\"\\nüìä File Counts:\")\n",
    "print(f\"   Human trace directories: {len(human_traces)}\")\n",
    "print(f\"   LLM model directories: {len(llm_models)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8fca40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   + Added 2000 AttackQA records\n",
      "üöÄ Harmonized 6118 samples into ../data/processed/agent_training_data_unified.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HARMONIZATION: EXPORT TO TRAINING FORMAT\n",
    "# ============================================================================\n",
    "\n",
    "TRAINING_DATA_PATH = PROCESSED_DIR / \"agent_training_data_unified.json\"\n",
    "\n",
    "def normalize_weblinx(row):\n",
    "    \"\"\"Convert WebLINX row to standard format\"\"\"\n",
    "    return {\n",
    "        \"id\": row['id'],\n",
    "        \"source\": \"weblinx\",\n",
    "        \"prompt\": row['utterances'], # You might need to parse this to get just the instruction\n",
    "        \"trace\": row['action_history'], # Already sequential\n",
    "        \"label\": 0 # Assume benign for now\n",
    "    }\n",
    "\n",
    "def normalize_tau2(row):\n",
    "    \"\"\"Convert TAU2 row to standard format\"\"\"\n",
    "    return {\n",
    "        \"id\": row['task_id'],\n",
    "        \"source\": \"tau2\",\n",
    "        \"prompt\": row['task_instructions'],\n",
    "        \"trace\": row['actions_required'], # These are gold-standard actions\n",
    "        \"label\": 0 # Assume benign (gold standard)\n",
    "    }\n",
    "\n",
    "def normalize_webarena_llm(row):\n",
    "    \"\"\"Convert WebArena LLM Trace to standard format\"\"\"\n",
    "    # row keys: trajectory_id, intent, actions, passed\n",
    "    return {\n",
    "        \"id\": row['trajectory_id'],\n",
    "        \"source\": \"webarena_llm\",\n",
    "        \"prompt\": row['intent'],\n",
    "        \"trace\": row['actions'],\n",
    "        \"label\": 1 if row['passed'] == 0 else 0 # FAIL might imply vulnerability or just incompetence. \n",
    "        # For adversarial training, you might care specifically about *successful* attacks. \n",
    "        # Adjust logic based on whether you want 'failed tasks' or 'successful attacks'.\n",
    "    }\n",
    "\n",
    "def normalize_fujitsu(row):\n",
    "    \"\"\"\n",
    "    Convert Fujitsu Red Teaming row to standard format.\n",
    "    \n",
    "    Mapping Strategy:\n",
    "    - prompt: The 'user_query' (this is the input vector containing the attack).\n",
    "    - trace: The 'target_output' (this shows the system failing/leaking).\n",
    "    - label: 1 (Adversarial/Unsafe).\n",
    "    - meta: Store the adversarial goal to understand intent.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"id\": row['unique_id'],\n",
    "        \"source\": f\"fujitsu_{row['dataset_subset']}\",\n",
    "        \"prompt\": row['user_query'],\n",
    "        \"trace\": row['target_output'], # The system's response to the attack\n",
    "        \"label\": 1, # These are explicitly adversarial/red-teaming examples\n",
    "        \"meta_intent\": row['adversarial_goal']\n",
    "    }\n",
    "def query_agent_dojo(limit: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Query AgentDojo samples\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM agent_dojo LIMIT {limit}\", conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "def normalize_agent_dojo(row):\n",
    "    \"\"\"\n",
    "    Convert AgentDojo row to standard format.\n",
    "    \n",
    "    Mapping Strategy:\n",
    "    - prompt: The user's request.\n",
    "    - trace: The extracted internal monologue (<thinking>) + tool usage.\n",
    "      (We use the thinking trace here as it provides unique insight into *why* the agent failed)\n",
    "    - label: 1 if it contains an injection attack, 0 otherwise.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"id\": row['unique_id'],\n",
    "        \"source\": \"agent_dojo\",\n",
    "        \"prompt\": row['prompt'],\n",
    "        \"trace\": row['thinking_trace'], # Utilizing the CoT data specifically\n",
    "        \"label\": 1 if row['is_attack'] else 0,\n",
    "        \"meta_security\": row['security'] # Did the agent survive the attack?\n",
    "    }\n",
    "# ============================================================================\n",
    "# HARMONIZATION: EXPORT TO TRAINING FORMAT\n",
    "# ============================================================================\n",
    "\n",
    "def query_attackqa(limit: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Query AttackQA samples\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM attack_qa LIMIT {limit}\", conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def normalize_attackqa(row):\n",
    "    \"\"\"\n",
    "    Convert AttackQA row to standard format.\n",
    "    Label 0: This is factual security knowledge, not an attack.\n",
    "    \"\"\"\n",
    "    trace_combined = f\"<thinking>{row['thought_trace']}</thinking>\\n{row['answer']}\" if row['thought_trace'] else row['answer']\n",
    "    \n",
    "    return {\n",
    "        \"id\": row['unique_id'],\n",
    "        \"source\": \"attack_qa\",\n",
    "        \"prompt\": row['question'],\n",
    "        \"trace\": trace_combined,\n",
    "        \"label\": 0, \n",
    "        \"meta_technique\": row['mitre_technique_id']\n",
    "    }\n",
    "\n",
    "# --- EXECUTE EXPORT ---\n",
    "\n",
    "all_training_data = []\n",
    "\n",
    "# 1. WebLINX\n",
    "df_weblinx = query_weblinx(limit=1000)\n",
    "all_training_data.extend([normalize_weblinx(r) for _, r in df_weblinx.iterrows()])\n",
    "\n",
    "# 2. TAU2\n",
    "df_tau2 = query_tau2('retail', limit=1000) \n",
    "all_training_data.extend([normalize_tau2(r) for _, r in df_tau2.iterrows()])\n",
    "\n",
    "# 3. WebArena\n",
    "df_wa = query_webarena_llm(limit=1000)\n",
    "all_training_data.extend([normalize_webarena_llm(r) for _, r in df_wa.iterrows()])\n",
    "\n",
    "# 4. Fujitsu (Red Teaming)\n",
    "df_fujitsu = query_fujitsu(limit=2000)\n",
    "all_training_data.extend([normalize_fujitsu(r) for _, r in df_fujitsu.iterrows()])\n",
    "\n",
    "# 5. AgentDojo (Red Teaming)\n",
    "try:\n",
    "    df_dojo = query_agent_dojo(limit=2000)\n",
    "    all_training_data.extend([normalize_agent_dojo(r) for _, r in df_dojo.iterrows()])\n",
    "except: pass\n",
    "\n",
    "# 6. AttackQA (Security Knowledge)\n",
    "df_aqa = query_attackqa(limit=2000)\n",
    "all_training_data.extend([normalize_attackqa(r) for _, r in df_aqa.iterrows()])\n",
    "print(f\"   + Added {len(df_aqa)} AttackQA records\")\n",
    "\n",
    "# Save\n",
    "with open(TRAINING_DATA_PATH, 'w') as f:\n",
    "    json.dump(all_training_data, f, indent=2)\n",
    "\n",
    "print(f\"üöÄ Harmonized {len(all_training_data)} samples into {TRAINING_DATA_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_adversarial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
