### **Info Panel: AttackQA**

**AttackQA** is not a red-teaming dataset; it is a **Domain Competency** dataset. It bridges the gap between general reasoning and specific cybersecurity knowledge.

### **Sample Trace (Fact Retrieval)**

*(From the dataset's "Technique" subset)*

```json
{
  "unique_id": "attackqa_T1539_0",
  "mitre_technique_id": "T1539", // "Steal Web Session Cookie"
  
  "question": "What is a potential indicator of the 'T1539: Steal Web Session Cookie' attack technique?",
  
  "thought_trace": "To answer the question, I need to understand how to detect the 'T1539: Steal Web Session Cookie' attack technique.",
  
  "answer": "A potential indicator of the 'T1539: Steal Web Session Cookie' attack technique is monitoring for attempts by programs to inject into or dump browser process memory.",
  
  "context_document": "How data component 'Process Access' can be used to detect attack technique 'T1539: Steal Web Session Cookie':\nMonitor for attempts by programs to inject into or dump browser process memory.",
  
  "is_human_question": false, // Generated by LLM
  "is_human_answer": false,   // Generated by LLM
  "source_relation": "relationships_detects"
}

```

### **Why is this dataset important?**

1. **Grounding for Security Agents:**
If you are training an "AI Security Analyst," it needs to know the difference between a user asking *"How do I hack this?"* (Attack) and *"What are the indicators of this hack?"* (Defense). AttackQA provides the **Defensive/Descriptive** language patterns.
2. **RAG Context:**
Every question in this dataset is paired with a `document` (the "Context" snippet). This makes it perfect for training the **retrieval** component of your agent. You can test if your agent retrieves the correct MITRE technique description before answering.
3. **Human vs. Synthetic:**
The dataset metadata explicitly flags `human_question` vs `human_answer`.
* **Human-Human:** High-quality, nuanced reasoning (Gold standard).
* **LLM-LLM:** High volume, good for bulk pre-training.
* **Hybrid:** Useful for testing alignment (e.g., Human asks, LLM answers).



### **Dataset Structure**

* **Size:** ~17,700 QA pairs.
* **Coverage:** Maps to the entire MITRE ATT&CK Enterprise Matrix.
* **Reasoning:** Unlike simple FAQ datasets, many records include a `thought` field, allowing you to train models to *reason* about security ("To answer this, I first need to check X...") rather than just memorizing facts.