"""
Circuit Breakers Trainer Module - Agentic Enhanced Version

Implements Representation Rerouting (RR) training for agent safety.
Based on "Improving Alignment and Robustness with Circuit Breakers" (Gray-Swan).

AGENTIC ENHANCEMENTS:
1. Completion-based training: Loss applied on assistant tokens, not just prompts
2. Token-level masking: cb_token_mask and retain_token_mask for targeted loss
3. Dual coefficient scheduling: cs(t) and cr(t) for paper-style loss weighting
4. Tool-call awareness: Handles agent traces with tool calls

Key components:
1. RepresentationExtractor: Hooks into model layers to extract hidden states
2. reroute_loss: ReLU(cosine_similarity) to push harmful representations orthogonal
3. retain_loss: L2 distance to preserve benign representations
4. CircuitBreakerTrainer: Main training loop with alpha schedule
"""

import json
import math
import os
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    get_linear_schedule_with_warmup,
    get_cosine_schedule_with_warmup,
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType,
)
from accelerate import Accelerator
from accelerate.utils import set_seed

from .config import CircuitBreakerConfig
from .hf_utils import resolve_hf_token
from scripts.utils.wandb_logging import (
    build_wandb_init_kwargs,
    config_to_dict_for_wandb,
    default_run_name,
    get_git_metadata,
    get_host_metadata,
    get_slurm_metadata,
    log_dir_as_artifact,
    parse_tags,
    wandb_is_available,
    write_wandb_run_ref,
)


# =============================================================================
# Constants for Agentic Training
# =============================================================================

# Patterns to identify assistant response boundaries
ASSISTANT_START_PATTERNS = [
    r"\[/INST\]",              # Llama style
    r"<\|assistant\|>",        # ChatML style
    r"<\|im_start\|>assistant", # Qwen style
    r"ASSISTANT:",             # Simple format
    r"assistant\n",            # Basic chat
]

# Tool call patterns (for agent traces)
TOOL_CALL_PATTERNS = [
    r"<tool_call>",
    r"<function_call>",
    r"\{\"tool\":",
    r"Action:\s*\w+",
    r"```tool_code",
]


# =============================================================================
# Representation Extraction
# =============================================================================

class RepresentationExtractor:
    """
    Extracts hidden state representations from specified layers during forward pass.
    
    Uses forward hooks to capture the output of transformer layers.
    """
    
    def __init__(self, model: nn.Module, target_layers: List[int]):
        """
        Args:
            model: The transformer model
            target_layers: List of layer indices to extract representations from
        """
        self.model = model
        self.target_layers = target_layers
        self.representations: Dict[int, torch.Tensor] = {}
        self.hooks = []
        
        self._register_hooks()
    
    def _get_layer_module(self, layer_idx: int) -> nn.Module:
        """Get the transformer layer module by index."""
        # Handle different model architectures
        if hasattr(self.model, 'model'):
            # PEFT wrapped model
            base_model = self.model.model
        else:
            base_model = self.model
            
        if hasattr(base_model, 'model'):
            # LlamaForCausalLM structure
            base_model = base_model.model
            
        if hasattr(base_model, 'layers'):
            return base_model.layers[layer_idx]
        elif hasattr(base_model, 'h'):
            # GPT-2 style
            return base_model.h[layer_idx]
        else:
            raise ValueError(f"Unknown model architecture: {type(base_model)}")
    
    def _register_hooks(self):
        """Register forward hooks on target layers."""
        for layer_idx in self.target_layers:
            layer = self._get_layer_module(layer_idx)
            
            def hook_fn(module, input, output, layer_idx=layer_idx):
                # Output is typically (hidden_states, ...) tuple
                if isinstance(output, tuple):
                    hidden_states = output[0]
                else:
                    hidden_states = output
                self.representations[layer_idx] = hidden_states
            
            hook = layer.register_forward_hook(hook_fn)
            self.hooks.append(hook)
    
    def get_representations(self) -> Dict[int, torch.Tensor]:
        """Get extracted representations from last forward pass."""
        return self.representations
    
    def clear(self):
        """Clear stored representations."""
        self.representations = {}
    
    def remove_hooks(self):
        """Remove all hooks."""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []


# =============================================================================
# Loss Functions
# =============================================================================

def reroute_loss(
    model_reps: Dict[int, torch.Tensor],
    frozen_reps: Dict[int, torch.Tensor],
    target_layers: List[int],
    attention_mask: Optional[torch.Tensor] = None,
    loss_mask: Optional[torch.Tensor] = None,
) -> torch.Tensor:
    """
    Rerouting Loss (L_rr): Push harmful representations to be orthogonal.

    Formula: L_rr = ReLU(cosine_similarity(h_model, h_frozen))

    This encourages the model's representation of harmful content to be
    orthogonal (cos_sim â‰ˆ 0) or opposite (cos_sim < 0) to the frozen model's.

    AGENTIC ENHANCEMENT: loss_mask allows applying loss only on specific tokens
    (e.g., assistant completion tokens, tool call arguments) rather than the
    full sequence. This is critical for completion-based training.

    Args:
        model_reps: Hidden states from trainable model {layer_idx: tensor}
        frozen_reps: Hidden states from frozen reference model
        target_layers: Which layers to compute loss on
        attention_mask: Optional mask to ignore padding tokens
        loss_mask: Optional mask for targeted loss (e.g., completion tokens only)
            If provided, loss is computed only on tokens where loss_mask=1

    Returns:
        Scalar loss tensor
    """
    total_loss = 0.0
    num_layers = 0

    # Combine attention_mask and loss_mask
    combined_mask = attention_mask
    if loss_mask is not None:
        if combined_mask is not None:
            combined_mask = combined_mask * loss_mask
        else:
            combined_mask = loss_mask

    for layer_idx in target_layers:
        if layer_idx not in model_reps or layer_idx not in frozen_reps:
            continue

        h_model = model_reps[layer_idx]  # (batch, seq_len, hidden_dim)
        h_frozen = frozen_reps[layer_idx]

        # Compute cosine similarity along hidden dimension
        # Normalize along last dimension
        h_model_norm = F.normalize(h_model, p=2, dim=-1)
        h_frozen_norm = F.normalize(h_frozen, p=2, dim=-1)

        # Cosine similarity per token
        cos_sim = (h_model_norm * h_frozen_norm).sum(dim=-1)  # (batch, seq_len)

        # Apply ReLU: only penalize positive similarity
        relu_cos = F.relu(cos_sim)

        # Apply combined mask if provided (ignore padding + non-target tokens)
        if combined_mask is not None:
            relu_cos = relu_cos * combined_mask.float()
            # Mean over masked tokens
            loss = relu_cos.sum() / (combined_mask.sum() + 1e-8)
        else:
            loss = relu_cos.mean()

        total_loss += loss
        num_layers += 1

    return total_loss / max(num_layers, 1)


def retain_loss(
    model_reps: Dict[int, torch.Tensor],
    frozen_reps: Dict[int, torch.Tensor],
    target_layers: List[int],
    attention_mask: Optional[torch.Tensor] = None,
    loss_mask: Optional[torch.Tensor] = None,
) -> torch.Tensor:
    """
    Retain Loss (L_ret): Preserve benign representations.

    Formula: L_ret = ||h_model - h_frozen||_2

    This encourages the model to keep benign representations close to the
    original frozen model's representations.

    AGENTIC ENHANCEMENT: loss_mask allows applying loss only on specific tokens
    (e.g., assistant completion tokens) rather than the full sequence.

    Args:
        model_reps: Hidden states from trainable model {layer_idx: tensor}
        frozen_reps: Hidden states from frozen reference model
        target_layers: Which layers to compute loss on
        attention_mask: Optional mask to ignore padding tokens
        loss_mask: Optional mask for targeted loss (e.g., completion tokens only)

    Returns:
        Scalar loss tensor
    """
    total_loss = 0.0
    num_layers = 0

    # Combine attention_mask and loss_mask
    combined_mask = attention_mask
    if loss_mask is not None:
        if combined_mask is not None:
            combined_mask = combined_mask * loss_mask
        else:
            combined_mask = loss_mask

    for layer_idx in target_layers:
        if layer_idx not in model_reps or layer_idx not in frozen_reps:
            continue

        h_model = model_reps[layer_idx]  # (batch, seq_len, hidden_dim)
        h_frozen = frozen_reps[layer_idx]

        # L2 distance per token
        l2_dist = torch.norm(h_model - h_frozen, p=2, dim=-1)  # (batch, seq_len)

        # Apply combined mask if provided
        if combined_mask is not None:
            l2_dist = l2_dist * combined_mask.float()
            loss = l2_dist.sum() / (combined_mask.sum() + 1e-8)
        else:
            loss = l2_dist.mean()

        total_loss += loss
        num_layers += 1

    return total_loss / max(num_layers, 1)


# =============================================================================
# Alpha Schedule
# =============================================================================

def get_alpha(
    step: int,
    alpha_max: float,
    total_steps: int,
    strategy: str = "linear",
    decay_multiplier: float = 2.0,
) -> float:
    """
    Compute alpha coefficient for the current step.

    Alpha controls the weight of the rerouting loss:
    - High alpha early: Aggressively push harmful representations away
    - Low alpha late: Focus on preserving benign capabilities

    Linear schedule: Î±(t) = Î±_max Ã— max(0, 1 - t / (2 Ã— total_steps))

    Args:
        step: Current training step
        alpha_max: Maximum alpha value
        total_steps: Total number of training steps
        strategy: "linear" or "cosine"

    Returns:
        Alpha value for this step
    """
    decay_steps = max(1, int(round(float(decay_multiplier) * total_steps)))

    if strategy == "linear":
        # Linear decay over (decay_multiplier * total_steps)
        alpha = alpha_max * max(0.0, 1.0 - step / decay_steps)
    elif strategy == "cosine":
        # Cosine decay
        progress = min(step / decay_steps, 1.0)
        alpha = alpha_max * 0.5 * (1.0 + math.cos(math.pi * progress))
    else:
        raise ValueError(f"Unknown alpha strategy: {strategy}")

    return alpha


def get_dual_coefficients(
    step: int,
    total_steps: int,
    alpha_max: float = 1.0,
    decay_multiplier: float = 2.0,
    strategy: str = "linear",
) -> Tuple[float, float]:
    """
    Compute dual coefficients cs(t) and cr(t) for paper-style loss weighting.

    The paper uses time-varying weights for both reroute and retain losses:
    L = (alpha_max * cs(t)) * L_rr + (alpha_max * cr(t)) * L_ret

    This shifts emphasis from rerouting early to retention late.

    Args:
        step: Current training step
        total_steps: Total training steps
        alpha_max: Maximum coefficient value
        decay_multiplier: How far past total_steps to decay
        strategy: "linear" or "cosine"

    Returns:
        (cs, cr) tuple where:
        - cs: coefficient for rerouting loss (starts high, decays)
        - cr: coefficient for retention loss (starts low, increases)
    """
    decay_steps = max(1, int(round(float(decay_multiplier) * total_steps)))
    progress = min(step / decay_steps, 1.0)

    if strategy == "linear":
        cs = max(0.0, 1.0 - progress)  # 1 -> 0
        cr = min(1.0, progress)         # 0 -> 1
    elif strategy == "cosine":
        # Smooth cosine transition
        cs = 0.5 * (1.0 + math.cos(math.pi * progress))  # 1 -> 0
        cr = 0.5 * (1.0 - math.cos(math.pi * progress))  # 0 -> 1
    else:
        raise ValueError(f"Unknown strategy: {strategy}")

    return alpha_max * cs, alpha_max * cr


# =============================================================================
# Token Masking Utilities (for completion-based training)
# =============================================================================

def find_assistant_start_position(text: str) -> int:
    """
    Find the character position where the assistant's response starts.

    Returns:
        Character position of assistant start, or 0 if not found
    """
    for pattern in ASSISTANT_START_PATTERNS:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return match.end()
    return 0


def create_completion_mask(
    input_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    tokenizer,
    text: str,
    mask_prompt: bool = True,
) -> torch.Tensor:
    """
    Create a mask that covers only the assistant completion tokens.

    For circuit breaker training, we want to apply:
    - reroute_loss on harmful COMPLETION tokens (not the user prompt)
    - retain_loss on benign COMPLETION tokens

    This ensures we're rerouting the model's generation trajectory,
    not just the input encoding.

    Args:
        input_ids: Token IDs (batch_size, seq_len)
        attention_mask: Attention mask
        tokenizer: Tokenizer for decoding
        text: Original text before tokenization
        mask_prompt: If True, mask out prompt tokens (keep only completion)

    Returns:
        Completion mask (batch_size, seq_len) where 1 = completion token
    """
    if not mask_prompt:
        return attention_mask.clone()

    # Find assistant start in text
    assistant_start_char = find_assistant_start_position(text)

    if assistant_start_char == 0:
        # No assistant marker found, use full sequence
        return attention_mask.clone()

    # Tokenize just the prompt part to find the split point
    prompt_part = text[:assistant_start_char]
    prompt_tokens = tokenizer.encode(prompt_part, add_special_tokens=False)
    prompt_len = len(prompt_tokens)

    # Create mask: 0 for prompt tokens, 1 for completion tokens
    batch_size, seq_len = input_ids.shape
    completion_mask = torch.zeros_like(attention_mask)

    for b in range(batch_size):
        # Account for special tokens at start
        start_idx = min(prompt_len + 1, seq_len)  # +1 for BOS if present
        completion_mask[b, start_idx:] = attention_mask[b, start_idx:]

    return completion_mask


def create_span_mask(
    seq_len: int,
    spans: List[Tuple[int, int]],
    device: torch.device = None,
) -> torch.Tensor:
    """
    Create a mask from explicit (start, end) token spans.

    Args:
        seq_len: Sequence length
        spans: List of (start, end) tuples
        device: Target device

    Returns:
        Mask tensor of shape (seq_len,)
    """
    mask = torch.zeros(seq_len, dtype=torch.float32, device=device)
    for start, end in spans:
        start = max(0, start)
        end = min(seq_len, end)
        mask[start:end] = 1.0
    return mask


# =============================================================================
# Hidden-state extraction (preferred)
# =============================================================================

def _select_hidden_states(
    outputs,
    target_layers: List[int],
) -> Dict[int, torch.Tensor]:
    """Select per-layer hidden states from a HF model output.

    In Transformers, outputs.hidden_states is a tuple of length (num_layers + 1):
      - hidden_states[0] is the embedding output
      - hidden_states[layer_idx + 1] is the output of transformer block layer_idx

    This helper returns a dict keyed by the *transformer block index* (layer_idx),
    matching the indexing used by the hook-based RepresentationExtractor.
    """
    hs = getattr(outputs, "hidden_states", None)
    if hs is None:
        raise ValueError("Model outputs missing hidden_states; pass output_hidden_states=True")

    reps: Dict[int, torch.Tensor] = {}
    for layer_idx in target_layers:
        hs_idx = layer_idx + 1
        if hs_idx < 0 or hs_idx >= len(hs):
            continue
        reps[layer_idx] = hs[hs_idx]
    return reps


# =============================================================================
# Dataset
# =============================================================================

class CircuitBreakerDataset(Dataset):
    """
    Dataset for Circuit Breaker training - Agentic Enhanced Version.

    Loads batches from cb_training_batches.jsonl where each batch contains:
    - harmful: List of 8 harmful samples with attack prompts/completions
    - benign: List of 8 benign samples with capability prompts/completions

    AGENTIC ENHANCEMENTS:
    1. Completion-aware: Prefers 'text' field with full prompt+completion
    2. Loss masking: Computes masks for completion-only loss computation
    3. Span support: Uses cb_token_mask/retain_token_mask if provided
    """

    def __init__(
        self,
        data_path: str,
        tokenizer,
        max_seq_length: int = 2048,
        mask_prompt_tokens: bool = True,
        use_chat_template: bool = True,
    ):
        """
        Args:
            data_path: Path to cb_training_batches.jsonl
            tokenizer: HuggingFace tokenizer
            max_seq_length: Maximum sequence length
            mask_prompt_tokens: If True, loss is computed only on completion tokens
            use_chat_template: If True, format prompts using chat template
        """
        self.tokenizer = tokenizer
        self.max_seq_length = max_seq_length
        self.mask_prompt_tokens = mask_prompt_tokens
        self.use_chat_template = use_chat_template
        self.batches = []

        # Load all batches
        with open(data_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line:
                    self.batches.append(json.loads(line))

        # Count completion-style examples
        completion_count = sum(
            1 for b in self.batches
            for s in b.get('harmful', []) + b.get('benign', [])
            if isinstance(s, dict) and s.get('text')
        )
        total_samples = sum(
            len(b.get('harmful', [])) + len(b.get('benign', []))
            for b in self.batches
        )

        print(f"Loaded {len(self.batches)} pre-batched training examples")
        print(f"  - Completion-style samples: {completion_count}/{total_samples} "
              f"({100*completion_count/max(1,total_samples):.1f}%)")
        if self.mask_prompt_tokens:
            print(f"  - Loss masking: ENABLED (completion tokens only)")

    def __len__(self):
        return len(self.batches)

    def _extract_text(self, sample: Dict[str, Any], is_harmful: bool = False) -> Tuple[str, bool]:
        """
        Extract the training text from a sample.

        Returns:
            (text, has_completion) tuple where has_completion indicates
            if this is a full prompt+completion example
        """
        # Priority 1: Pre-rendered full text (prompt + completion)
        if isinstance(sample, dict) and sample.get('text'):
            return str(sample['text']), True

        # Priority 2: Separate prompt and completion fields
        if isinstance(sample, dict):
            prompt = sample.get('user_prompt', '')
            completion = sample.get('harmful_completion' if is_harmful else 'benign_completion', '')

            if prompt and completion:
                # Format with chat template if available
                if self.use_chat_template and hasattr(self.tokenizer, 'apply_chat_template'):
                    messages = [
                        {"role": "user", "content": prompt},
                        {"role": "assistant", "content": completion}
                    ]
                    try:
                        text = self.tokenizer.apply_chat_template(
                            messages, tokenize=False, add_generation_prompt=False
                        )
                        return text, True
                    except Exception:
                        pass

                # Fallback: simple concatenation
                text = f"User: {prompt}\n\nAssistant: {completion}"
                return text, True

        # Priority 3: Prompt-only (legacy format)
        prompt = self._extract_prompt_legacy(sample)
        return prompt, False

    def _extract_prompt_legacy(self, sample: Dict[str, Any]) -> str:
        """Extract prompt from legacy format samples."""
        if isinstance(sample, dict) and 'user_prompt' in sample and sample['user_prompt']:
            return str(sample['user_prompt'])

        if 'attack_prompt' in sample:
            # Fujitsu format: combine benign query with attack
            prompt = sample.get('benign_query', '') + '\n' + sample['attack_prompt']
        elif 'prompt' in sample:
            prompt = sample['prompt']
        elif 'messages' in sample:
            # AgentDojo format: use first user message
            messages = sample['messages']
            for m in messages:
                if m.get('role') == 'user':
                    prompt = m.get('content', '')
                    break
            else:
                prompt = str(messages[0].get('content', '')) if messages else ''
        else:
            prompt = str(sample)

        return prompt

    def _compute_completion_mask(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        texts: List[str],
        has_completions: List[bool],
    ) -> torch.Tensor:
        """
        Compute completion-only mask for a batch of sequences.

        Args:
            input_ids: (batch_size, seq_len)
            attention_mask: (batch_size, seq_len)
            texts: Original texts before tokenization
            has_completions: Whether each sample has a completion

        Returns:
            loss_mask: (batch_size, seq_len) with 1s only on completion tokens
        """
        if not self.mask_prompt_tokens:
            return attention_mask.clone()

        batch_size, seq_len = input_ids.shape
        loss_mask = torch.zeros_like(attention_mask)

        for i in range(batch_size):
            if not has_completions[i]:
                # No completion available, use full sequence
                loss_mask[i] = attention_mask[i]
                continue

            # Find where completion starts in text
            text = texts[i]
            completion_start_char = find_assistant_start_position(text)

            if completion_start_char == 0:
                # Couldn't find assistant marker, use full sequence
                loss_mask[i] = attention_mask[i]
                continue

            # Tokenize prompt portion to find split point
            prompt_text = text[:completion_start_char]
            try:
                prompt_tokens = self.tokenizer.encode(
                    prompt_text, add_special_tokens=True
                )
                prompt_len = len(prompt_tokens)
            except Exception:
                prompt_len = 0

            # Create mask: 0 for prompt, 1 for completion
            if prompt_len > 0 and prompt_len < seq_len:
                loss_mask[i, prompt_len:] = attention_mask[i, prompt_len:]

        return loss_mask

    def __getitem__(self, idx):
        """
        Get a batch of harmful and benign samples.

        Returns:
            Dict with tokenized inputs and optional loss masks
        """
        batch = self.batches[idx]

        # Extract texts from harmful and benign samples
        harmful_data = [
            self._extract_text(s, is_harmful=True)
            for s in batch['harmful']
        ]
        harmful_texts = [t[0] for t in harmful_data]
        harmful_has_completions = [t[1] for t in harmful_data]

        benign_data = [
            self._extract_text(s, is_harmful=False)
            for s in batch['benign']
        ]
        benign_texts = [t[0] for t in benign_data]
        benign_has_completions = [t[1] for t in benign_data]

        # Tokenize harmful
        harmful_tokens = self.tokenizer(
            harmful_texts,
            padding='max_length',
            truncation=True,
            max_length=self.max_seq_length,
            return_tensors='pt',
        )

        # Tokenize benign
        benign_tokens = self.tokenizer(
            benign_texts,
            padding='max_length',
            truncation=True,
            max_length=self.max_seq_length,
            return_tensors='pt',
        )

        result = {
            'harmful_input_ids': harmful_tokens['input_ids'],
            'harmful_attention_mask': harmful_tokens['attention_mask'],
            'benign_input_ids': benign_tokens['input_ids'],
            'benign_attention_mask': benign_tokens['attention_mask'],
        }

        # Compute completion masks for targeted loss
        if self.mask_prompt_tokens:
            result['harmful_loss_mask'] = self._compute_completion_mask(
                harmful_tokens['input_ids'],
                harmful_tokens['attention_mask'],
                harmful_texts,
                harmful_has_completions,
            )
            result['benign_loss_mask'] = self._compute_completion_mask(
                benign_tokens['input_ids'],
                benign_tokens['attention_mask'],
                benign_texts,
                benign_has_completions,
            )

        # Include explicit span masks if provided in data
        for key in ['cb_token_mask', 'retain_token_mask']:
            if any(isinstance(s, dict) and key in s for s in batch.get('harmful', [])):
                # Use provided masks (for advanced use cases)
                pass  # TODO: Implement explicit span mask loading

        return result


def collate_fn(batch):
    """Collate function for DataLoader (batch of 1 pre-batched item)."""
    # Since each item is already a batch, just return it
    return batch[0]


# =============================================================================
# Trainer
# =============================================================================

class CircuitBreakerTrainer:
    """
    Main trainer for Circuit Breaker (Representation Rerouting).
    
    Training loop:
    1. Load batch with harmful and benign samples
    2. Forward pass through trainable model, extract representations
    3. Forward pass through frozen model (no grad), extract representations
    4. Compute reroute_loss on harmful samples
    5. Compute retain_loss on benign samples
    6. Combined loss = Î±(t) Ã— reroute_loss + retain_loss
    7. Backward pass and optimizer step
    """
    
    def __init__(self, config: CircuitBreakerConfig):
        self.config = config

        # Resolve whether W&B is actually usable (package present).
        if self.config.use_wandb and not wandb_is_available():
            self.config.use_wandb = False

        self.accelerator = Accelerator(
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            log_with="wandb" if self.config.use_wandb else None,
            project_dir=self.config.output_dir,
        )
        
        # Set seed for reproducibility
        set_seed(42)
        
        # Initialize logging (rank-0 only)
        if self.config.use_wandb and self.accelerator.is_main_process:
            repo_dir = Path(__file__).resolve().parents[2]
            slurm_meta = get_slurm_metadata()
            host_meta = get_host_metadata()
            git_meta = get_git_metadata(repo_dir)

            # Derive defaults from env when not explicitly configured.
            env_group = os.environ.get("WANDB_GROUP")
            env_entity = os.environ.get("WANDB_ENTITY")
            env_tags = parse_tags(os.environ.get("WANDB_TAGS"))
            env_mode = os.environ.get("WANDB_MODE")

            if not self.config.wandb_run_name:
                self.config.wandb_run_name = default_run_name(
                    base_model=self.config.base_model,
                    total_steps=self.config.total_steps,
                )

            init_kwargs = build_wandb_init_kwargs(
                run_name=self.config.wandb_run_name,
                group=self.config.wandb_group or env_group,
                entity=self.config.wandb_entity or env_entity,
                tags=(self.config.wandb_tags or env_tags),
                notes=self.config.wandb_notes,
                dir_path=os.environ.get("WANDB_DIR"),
                mode=self.config.wandb_mode or env_mode,
            )

            wb_config = config_to_dict_for_wandb(self.config)
            wb_config.update({
                "slurm": slurm_meta,
                "host": host_meta,
                **git_meta,
            })

            self.accelerator.init_trackers(
                project_name=self.config.wandb_project,
                config=wb_config,
                init_kwargs=init_kwargs,
            )

            write_wandb_run_ref(Path(self.config.output_dir))
        
        # Load tokenizer (HF gated models require an auth token)
        hf_token = resolve_hf_token()
        self.tokenizer = AutoTokenizer.from_pretrained(
            config.base_model,
            token=hf_token,
            trust_remote_code=True,
        )
        # Right padding is typical for causal LM training.
        if getattr(self.tokenizer, "padding_side", None) != "right":
            self.tokenizer.padding_side = "right"
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Load models
        self._load_models()
        
        # Setup dataset and dataloader
        self._setup_data()
        
        # Setup optimizer and scheduler
        self._setup_optimizer()
        
        # Prepare with accelerator
        self._prepare_accelerator()
        
        # Setup representation extractors
        self._setup_extractors()
        
        self.global_step = 0
    
    def _config_to_dict(self) -> dict:
        """Convert config to dict for logging."""
        from dataclasses import asdict
        return asdict(self.config)
    
    def _load_models(self):
        """Load trainable and frozen reference models."""
        self.accelerator.print(f"Loading model: {self.config.base_model}")

        hf_token = resolve_hf_token()
        
        # Determine dtype
        dtype_map = {
            "bfloat16": torch.bfloat16,
            "float16": torch.float16,
            "float32": torch.float32,
        }
        torch_dtype = dtype_map.get(self.config.torch_dtype, torch.bfloat16)
        
        # Load trainable model with LoRA
        # Under multi-GPU Accelerate/DDP, keep device_map=None and let Accelerate place shards.
        # device_map="auto" is only safe for single-process inference-style loading.
        device_map = "auto" if self.accelerator.num_processes == 1 else None

        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.base_model,
            torch_dtype=torch_dtype,
            device_map=device_map,
            trust_remote_code=True,
            token=hf_token,
        )
        
        if self.config.gradient_checkpointing:
            self.model.gradient_checkpointing_enable()
        
        # Setup LoRA
        lora_config = LoraConfig(
            r=self.config.lora.r,
            lora_alpha=self.config.lora.alpha,
            lora_dropout=self.config.lora.dropout,
            target_modules=self.config.lora.target_modules,
            layers_to_transform=self.config.lora.target_layers,
            task_type=TaskType.CAUSAL_LM,
            bias="none",
        )
        
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        
        # Load frozen reference model (for computing loss targets)
        self.accelerator.print("Loading frozen reference model...")
        self.frozen_model = AutoModelForCausalLM.from_pretrained(
            self.config.base_model,
            torch_dtype=torch_dtype,
            device_map=device_map,
            trust_remote_code=True,
            token=hf_token,
        )
        self.frozen_model.eval()
        for param in self.frozen_model.parameters():
            param.requires_grad = False
    
    def _setup_data(self):
        """Setup dataset and dataloader."""
        # Get config options for completion-based training
        mask_prompt_tokens = getattr(self.config, 'mask_prompt_tokens', True)
        use_chat_template = getattr(self.config, 'use_chat_template', True)

        self.dataset = CircuitBreakerDataset(
            data_path=self.config.data_path,
            tokenizer=self.tokenizer,
            max_seq_length=self.config.max_seq_length,
            mask_prompt_tokens=mask_prompt_tokens,
            use_chat_template=use_chat_template,
        )

        self.dataloader = DataLoader(
            self.dataset,
            batch_size=1,  # Each item is already a batch
            shuffle=True,
            collate_fn=collate_fn,
            num_workers=4,
            pin_memory=True,
        )
    
    def _setup_optimizer(self):
        """Setup optimizer and learning rate scheduler."""
        # Only optimize LoRA parameters
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        
        self.optimizer = torch.optim.AdamW(
            trainable_params,
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
        )
        
        total_steps = min(self.config.total_steps, len(self.dataloader))
        
        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=self.config.warmup_steps,
            num_training_steps=total_steps,
        )
    
    def _prepare_accelerator(self):
        """Prepare models and optimizer with accelerator for multi-GPU."""
        self.model, self.frozen_model, self.optimizer, self.dataloader, self.scheduler = \
            self.accelerator.prepare(
                self.model, self.frozen_model, self.optimizer, self.dataloader, self.scheduler
            )
    
    def _setup_extractors(self):
        """Setup representation extractors for both models."""
        method = (self.config.representation_extraction or "").strip().lower()
        if method not in {"hidden_states", "hooks"}:
            raise ValueError(
                f"Unknown representation_extraction: {self.config.representation_extraction}. "
                "Use 'hidden_states' or 'hooks'."
            )

        self._rep_extraction_method = method

        if method == "hooks":
            # Get the underlying model (unwrap accelerator and PEFT)
            unwrapped_model = self.accelerator.unwrap_model(self.model)
            unwrapped_frozen = self.accelerator.unwrap_model(self.frozen_model)

            self.model_extractor = RepresentationExtractor(
                unwrapped_model, self.config.cb_target_layers
            )
            self.frozen_extractor = RepresentationExtractor(
                unwrapped_frozen, self.config.cb_target_layers
            )
        else:
            # No hooks required.
            self.model_extractor = None
            self.frozen_extractor = None
    
    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """
        Execute one training step - Agentic Enhanced Version.

        AGENTIC ENHANCEMENTS:
        1. Uses loss masks for completion-only loss computation
        2. Supports dual coefficient scheduling (cs/cr)

        Args:
            batch: Dict with harmful_input_ids, harmful_attention_mask,
                   benign_input_ids, benign_attention_mask,
                   and optionally harmful_loss_mask, benign_loss_mask

        Returns:
            Dict with loss values for logging
        """
        self.model.train()

        # Get coefficients for this step
        use_dual = getattr(self.config, 'loss_weighting', 'single_alpha') == 'dual'

        if use_dual:
            # Paper-style dual coefficients: cs decays, cr increases
            cs, cr = get_dual_coefficients(
                self.global_step,
                self.config.total_steps,
                self.config.alpha_max,
                self.config.alpha_decay_multiplier,
                self.config.alpha_decay_strategy,
            )
            alpha = cs  # For logging compatibility
        else:
            # Original single alpha schedule
            alpha = get_alpha(
                self.global_step,
                self.config.alpha_max,
                self.config.total_steps,
                self.config.alpha_decay_strategy,
                self.config.alpha_decay_multiplier,
            )
            cs, cr = alpha, 1.0  # cr fixed at 1.0 for single_alpha mode

        # === Process Harmful Samples (Rerouting) ===
        if self._rep_extraction_method == "hooks":
            self.model_extractor.clear()
            self.frozen_extractor.clear()

        harmful_input_ids = batch['harmful_input_ids']
        harmful_attention_mask = batch['harmful_attention_mask']
        harmful_loss_mask = batch.get('harmful_loss_mask', None)

        if self._rep_extraction_method == "hooks":
            # Forward through trainable model
            _ = self.model(
                input_ids=harmful_input_ids,
                attention_mask=harmful_attention_mask,
                use_cache=False,
            )
            harmful_model_reps = self.model_extractor.get_representations()

            # Forward through frozen model (no grad)
            with torch.no_grad():
                _ = self.frozen_model(
                    input_ids=harmful_input_ids,
                    attention_mask=harmful_attention_mask,
                    use_cache=False,
                )
            harmful_frozen_reps = self.frozen_extractor.get_representations()
        else:
            outputs = self.model(
                input_ids=harmful_input_ids,
                attention_mask=harmful_attention_mask,
                output_hidden_states=True,
                use_cache=False,
                return_dict=True,
            )
            harmful_model_reps = _select_hidden_states(outputs, self.config.cb_target_layers)
            del outputs

            with torch.no_grad():
                frozen_outputs = self.frozen_model(
                    input_ids=harmful_input_ids,
                    attention_mask=harmful_attention_mask,
                    output_hidden_states=True,
                    use_cache=False,
                    return_dict=True,
                )
                harmful_frozen_reps = _select_hidden_states(
                    frozen_outputs, self.config.cb_target_layers
                )
            del frozen_outputs

        # Compute rerouting loss with optional loss mask
        loss_reroute = reroute_loss(
            harmful_model_reps,
            harmful_frozen_reps,
            self.config.cb_target_layers,
            harmful_attention_mask,
            loss_mask=harmful_loss_mask,
        )

        # === Process Benign Samples (Retain) ===
        if self._rep_extraction_method == "hooks":
            self.model_extractor.clear()
            self.frozen_extractor.clear()

        benign_input_ids = batch['benign_input_ids']
        benign_attention_mask = batch['benign_attention_mask']
        benign_loss_mask = batch.get('benign_loss_mask', None)

        if self._rep_extraction_method == "hooks":
            # Forward through trainable model
            _ = self.model(
                input_ids=benign_input_ids,
                attention_mask=benign_attention_mask,
                use_cache=False,
            )
            benign_model_reps = self.model_extractor.get_representations()

            # Forward through frozen model (no grad)
            with torch.no_grad():
                _ = self.frozen_model(
                    input_ids=benign_input_ids,
                    attention_mask=benign_attention_mask,
                    use_cache=False,
                )
            benign_frozen_reps = self.frozen_extractor.get_representations()
        else:
            outputs = self.model(
                input_ids=benign_input_ids,
                attention_mask=benign_attention_mask,
                output_hidden_states=True,
                use_cache=False,
                return_dict=True,
            )
            benign_model_reps = _select_hidden_states(outputs, self.config.cb_target_layers)
            del outputs

            with torch.no_grad():
                frozen_outputs = self.frozen_model(
                    input_ids=benign_input_ids,
                    attention_mask=benign_attention_mask,
                    output_hidden_states=True,
                    use_cache=False,
                    return_dict=True,
                )
                benign_frozen_reps = _select_hidden_states(
                    frozen_outputs, self.config.cb_target_layers
                )
            del frozen_outputs

        # Compute retain loss with optional loss mask
        loss_retain = retain_loss(
            benign_model_reps,
            benign_frozen_reps,
            self.config.cb_target_layers,
            benign_attention_mask,
            loss_mask=benign_loss_mask,
        )

        # === Combined Loss ===
        # L = cs * L_reroute + cr * L_retain
        total_loss = cs * loss_reroute + cr * loss_retain

        # Backward pass
        self.accelerator.backward(total_loss)

        # Gradient clipping
        if self.config.max_grad_norm > 0:
            self.accelerator.clip_grad_norm_(
                self.model.parameters(),
                self.config.max_grad_norm,
            )

        # Optimizer step
        self.optimizer.step()
        self.scheduler.step()
        self.optimizer.zero_grad()

        metrics = {
            'loss': total_loss.item(),
            'loss_reroute': loss_reroute.item(),
            'loss_retain': loss_retain.item(),
            'alpha': alpha,
            'lr': self.scheduler.get_last_lr()[0],
        }

        # Add dual coefficient logging if enabled
        if use_dual:
            metrics['cs'] = cs
            metrics['cr'] = cr

        return metrics
    
    def train(self):
        """Main training loop."""
        self.accelerator.print("=" * 60)
        self.accelerator.print("Starting Circuit Breaker Training")
        self.accelerator.print(f"  Model: {self.config.base_model}")
        self.accelerator.print(f"  Total Steps: {self.config.total_steps}")
        self.accelerator.print(f"  Alpha Max: {self.config.alpha_max}")
        self.accelerator.print(f"  CB Target Layers: {self.config.cb_target_layers}")
        self.accelerator.print("=" * 60)
        
        progress_bar = tqdm(
            total=self.config.total_steps,
            desc="Training",
            disable=not self.accelerator.is_main_process,
        )
        
        epoch = 0
        while self.global_step < self.config.total_steps:
            epoch += 1
            self.accelerator.print(f"\n--- Epoch {epoch} ---")
            
            for batch in self.dataloader:
                if self.global_step >= self.config.total_steps:
                    break
                
                # Move batch to device
                batch = {k: v.to(self.accelerator.device) for k, v in batch.items()}
                
                # Training step
                metrics = self.train_step(batch)
                
                self.global_step += 1
                progress_bar.update(1)
                
                # Logging
                if self.global_step % self.config.logging_steps == 0:
                    self.accelerator.print(
                        f"Step {self.global_step}: loss={metrics['loss']:.4f}, "
                        f"reroute={metrics['loss_reroute']:.4f}, "
                        f"retain={metrics['loss_retain']:.4f}, "
                        f"Î±={metrics['alpha']:.4f}"
                    )
                    
                    if self.config.use_wandb:
                        if self.accelerator.is_main_process:
                            self.accelerator.log(metrics, step=self.global_step)
                
                # Save checkpoint
                if self.global_step % self.config.save_steps == 0:
                    self.save_checkpoint()
        
        progress_bar.close()
        
        # Final save
        self.save_checkpoint(final=True)
        
        if self.config.use_wandb and self.accelerator.is_main_process:
            self.accelerator.end_training()
        
        self.accelerator.print("\nâœ… Training complete!")
    
    def save_checkpoint(self, final: bool = False):
        """Save model checkpoint."""
        self.accelerator.wait_for_everyone()
        
        if self.accelerator.is_main_process:
            output_dir = Path(self.config.output_dir)
            if final:
                save_path = output_dir / "final"
            else:
                save_path = output_dir / f"checkpoint-{self.global_step}"
            
            save_path.mkdir(parents=True, exist_ok=True)
            
            # Save LoRA weights
            unwrapped_model = self.accelerator.unwrap_model(self.model)
            unwrapped_model.save_pretrained(save_path)
            self.tokenizer.save_pretrained(save_path)
            
            self.accelerator.print(f"ðŸ’¾ Saved checkpoint to {save_path}")

            # Optional W&B artifact logging (final checkpoint only by default).
            if (
                self.config.use_wandb
                and final
                and getattr(self.config, "wandb_log_artifacts", "none") == "final"
            ):
                artifact_name = f"cb-{self.config.wandb_run_name or 'run'}-final"
                log_dir_as_artifact(
                    artifact_name=artifact_name,
                    artifact_type=getattr(self.config, "wandb_artifact_type", "model"),
                    dir_path=save_path,
                    aliases=["final"],
                    metadata={
                        "global_step": self.global_step,
                        "output_dir": str(self.config.output_dir),
                        "base_model": self.config.base_model,
                    },
                )
    
    def cleanup(self):
        """Cleanup hooks and resources."""
        if getattr(self, "_rep_extraction_method", None) == "hooks":
            if self.model_extractor is not None:
                self.model_extractor.remove_hooks()
            if self.frozen_extractor is not None:
                self.frozen_extractor.remove_hooks()
